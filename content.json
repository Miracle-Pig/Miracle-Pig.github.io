{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[{"title":"Repositories","date":"2022-05-16T12:27:02.457Z","updated":"2022-05-16T12:27:02.457Z","comments":false,"path":"repository/index.html","permalink":"http://example.com/repository/index.html","excerpt":"","text":"Your Repositories (github.com)"}],"posts":[{"title":"Operating System","slug":"Operating System","date":"2022-05-16T15:46:36.945Z","updated":"2022-05-16T16:23:30.789Z","comments":true,"path":"2022/05/16/Operating System/","link":"","permalink":"http://example.com/2022/05/16/Operating%20System/","excerpt":"","text":"面试什么是操作系统 运行在计算机上的程序 管理计算机硬件和软件资源 屏蔽了硬件层的复杂性 内核负责系统的内存管理、硬件设备的管理、文件系统的管理以及应用程序的管理 系统调用 用户态 K用户态运行的进程可直接读取用户数据 内核态 内核态运行的进程能访问系统一切的内核资源在用户态运行程序时，凡是和内核态资源相关的操作，都必须通过系统调用的方式向操作系统提出服务请求，并由操作系统代为完成 系统调用按功能大致分为如下几类设备管理。完成设备的请求或释放，以及设备启动等功能。文件管理。完成文件的读、写、创建及删除等功能。进程控制。完成进程的创建、撤销、阻塞及唤醒等功能。进程通信。完成进程之间的消息传递或信号传递等功能。内存管理。完成内存的分配、回收以及获取作业占用内存区大小及地址等功能。 进程和线程 联系与区别 一个进程可存在多个线程，线程是进程划分成的更小的运行单位；多个线程共享进程的堆区、用户代码区、静态存储区，每个线程拥有各自独立的栈区、寄存器、程序计数器；多个进程相互独立、而多个线程会相互影响。 进程状态： 创建：进程正在被创建，尚未到就绪状态 就绪：进程处于准备运行状态，拥有除处理机外的一切 资源，即当拥有处理机资源时会立即进入运行状态 阻塞：进程等待某一事件而暂停运行如等待某资源为可用或等待IO操作完成。 运行：进程正在处理器上运行 结束：进程正在系统中消失 进程间通信方式 管道 匿名：具有亲情关系的进程间通信，存在于内存中 有名：先进先出，以磁盘文件形式存在 信号 较为复杂的通信方式，通知或接受进程某个事件发生 消息队列： 进程间消息的链表 存在于内存中，使用消息队列标志进行标识通信时先进先出 读取时可随机读取，也可按消息的类型读取克服了信号承载信息量少、管道只能承载无格式字节流、缓冲区大小受限等缺点信号量 进程间共享资源的计数器，意图在于进程间同步，避免竞争条件共享内存 多个进程访问同一块内存空间，不同进程可及时看到对方进程中对共享数据的更新 需要互斥锁、信号量等同步操作进行协调最有效的通信方式 套接字 客户端进程与服务端进程之间进行通信TCP&#x2F;IP的基本操作单元 不同主机之间的进程进行双向通信线程间的同步方式 在两个或多个具有共享资源的线程并发执行时，使用同步方式避免关键资源的使用冲突 三种同步方式 互斥量 通过使用互斥对象机制，只有拥有互斥对象的线程才有资格访问共享资源 互斥对象只有一个，可保证公共资源不会被多个线程同时访问 信号量 多个线程在同一时刻可以同时访问共享资源，但访问资源的最大线程数会被限制 PV操作 以通知的方式来保持多线程同步，可方便的实现多线程优先级的比较操作 进程调度 先来先服务：从就绪队列中选择最先进入该队列的进程为之分配资源 balady异常 短作业优先：从就绪队列中选择估计执行时间最短的进程为之分配资源 优先级调度：从就绪队列中选择最高优先级的进程为之分配资源 时间片轮转：每个进程被分配一个时间段，称作它的时间片，即该进程允许运行的时间 多级反馈队列：既能使高优先级的作业及时得到响应又能使短作业（进程）迅速完成，最好的调度算法，UNIX采取的使这种 死锁 概念：多个进程&#x2F;线程同时被阻塞，它们中的一个或全部都在等待某个资源被释放，由于进程&#x2F;线程被无限期地阻塞，因此程序不能正常终止 必要条件 互斥条件：共享资源在同一时刻只能被一个进程使用 不可剥夺条件：进程当前所拥有地资源不能被其他进程抢占请求和保持条件：一个进程拥有资源且要申请其他资源（这个资源被其他进程占有） 循环等待条件：有一组进程{P0，P1…，Pn}，P0等待的进程被P1占有。。。 解决死锁的方法 预防：采用某种策略，限制并发进程对资源的请求，提前破坏必要条件 破坏互斥条件：资源可同时访问，最简单的方法，但有些资源禁止同时访问 破坏不可剥夺条件：剥夺式调度算法，仅适用于主存资源和处理器资源，会导致资源利用率下降 破坏请求和保持条件（静态分配策略）：进程在执行前就申请到它所需要的全部资源，申请不到则不占有资源， 破坏循环等待条件（层次分配策略）：一个进程只能请求和释放高层次的资源，这样就不会出现循环等待链 避免：系统在分配资源时，根据使用情况提前做出预测，避免死锁发生 将系统分为安全状态和不安全状态 若系统将资源分配给申请者会产生死锁（不安全状态），则拒绝分配；否则（安全状态）接受申请，分配资源 通过Dijkstra的银行家算法，保持系统安全状态：当一个进程申请使用资源的时候，银行家算法试探性地分配该进程资源，然后通过安全性算法判断分配后地系统是否处于安全状态，若不安全则试探分配作废，让该进程继续等待，若能够进入安全状态，则就真分配资源给该进程 改善解决了资源使用率低的问题，但需要检测每个进程资源占用和申请情况，以及安全性检测，需要花费较多的时间检测：系统设定专门的机构，死锁发生时，该机构能够检测死锁的发生，并精确地确定与死锁有关地进程和资源 对资源地分配加以限制可预防和避免死锁地发生，但不利于资源在进程之间充分共享 死锁的检测不去提前预防和避免死锁的发生，而是直接分配资源，当程序发生死锁时，检测死锁发生的进程和其资源，之后采用特定方法进行解除 使用进程-资源分配图检测系统是否处于死锁状态没有环路时，则系统不存在死锁状态 如果进程-资源分配图中有环路，且每个资源类仅有一个资源，则系统中已经发生了死锁。 如果进程-资源分配图中有环路，且涉及到的资源类有多个资源，此时系统未必会发生死锁。如果能在进程-资源分配图中找出一个 既不阻塞又非独立的进程 ，该进程能够在有限的时间内归还占有的资源，也就是把边给消除掉了，重复此过程，直到能在有限的时间内 消除所有的边 ，则不会发生死锁，否则会发生死锁。(消除边的过程类似于 拓扑排序)解除：四种方法 立即结束所有进程，重启系统：方法简单，损失较大撤销死锁进程，解除后继续运行：损失同样较大逐个撤销涉及死锁的进程，回收其资源直至死锁解除 抢占资源 操作系统内存管理基础 连续分配 块式：将内存分为多个固定大小的块，每个块包含一个进程，会产生较大的内存碎片 非连续分配 页式：将主存分为大小相等且固定的一页一页的形式，页无实际意义，页较小，较块式划分粒度更小，提高内存利用率，页式管理通过页表将逻辑地址转换为物理地址 段式：把主存分为一段一段，段有实际意义，每个段定义了一组逻辑信息，例如将一个程序分为主程序段、子程序段、数据段、栈段等 操作系统内存管理基础逻辑(虚拟)地址和物理地址 虚拟地址 进程可用的虚拟地址范围成为虚拟地址空间，编程只可能和逻辑地址打交道 虚拟地址&#x3D;虚拟页号+页内偏移量 物理地址 真实物理内存中的地址，就是内存地址寄存器中的地址 物理地址&#x3D;物理页号+页内偏移量块表和多级页表 分页内存管理，最重要的两点 虚拟地址到物理地址的转换要快 解决虚拟地址空间越大页表越大的问题 块表 一种特殊的高速缓冲存储器（cache） 在有块表存在的情况下，虚拟地址转换为物理地址的流程 根据虚拟页号查询快表 若存在虚拟页号和物理页号映射的快表项，则直接取得物理页号和页内偏移量拼接形成物理地址（读写数据时，访问一次cache和一次主存） 若不存在，则访问页表，找到虚拟页号和物理页号映射的页表项，取得物理页号和页内偏移量拼接形成物理地址，之后将此映射添加到快表中，若快表已满，则使用一定淘汰策略替换某个快表项（读写数据时，访问一次cache和两次主存） 多级页表 时间换空间 使用多级页表避免将所有页表项都放入内存，有些不必要的页表项不必要保留在内存中。 分页机制和分段机制的共同点和区别 共同点 都是为了提高内存的利用率，减少内存碎片 都是不连续（离散）分配方式，但在页&#x2F;段内为连续分配 区别 页为固定大小；段大小不固定，取决于当前程序 页只是单纯的满足OS内存管理的需求，每一页不代表任何意义；段是程序逻辑信息的单位，在程序中可体现为代码段、数据段满足用户需求 CPU虚拟寻址 将虚拟地址转换为物理地址 CPU内存管理单元进行操作 为什么要有虚拟地址空间呢？ 直接访问内存空间，会对操作系统造成伤害 不能同时运行多个程序 优势： 使用虚拟地址空间通过页表可使用内存中不相邻的大内存 程序可使用一系列虚拟地址来访问大于可用物理内存的内存缓冲区，当物理内存的供应量变小时，内存管理器会将物理页保存到磁盘文件。数据或代码页会根据需要在物理内存与磁盘文件之间移动 虚拟内存 OS中许多软件，这些软件“需要”占用的内存远远大于实际的物理内存 通过虚拟内存可让程序可拥有超过系统物理内存大小的可用内存空间 虚拟内存为每个进程提供了一个一致的、私有的地址空间，让每个进程产生一种自己在独享主存的错觉（每个进程拥有一片连续完整的内存空间）局部性原理 时间局部性：如果程序中的某条指令一旦执行，不久以后该指令可能再次执行；如果某数据被访问过，不久以后该数据可能再次被访问。产生时间局部性的典型原因，是由于在程序中存在着大量的循环操作。 空间局部性 ：一旦程序访问了某个存储单元，在不久之后，其附近的存2储单元也将被访问，即程序在一段时间内所访问的地址，可能集中在一定的范围之内，这是因为指令通常是顺序存放、顺序执行的，数据也一般是以向量、数组、表等形式簇聚存储的。虚拟存储器 程序在运行时所需要的内存比实际占用的内存要大得多，虚拟存储器只将程序需要的一部分调入内存，其他部分仍在外存。当程序访问到的数据不在内存时，由OS将所需要的部分调入内存。OS将内存中暂时不使用的内容换到外存上，腾出空间存放将要调入内存的信息。虚拟内存的技术实现 请求分页：程序运行之前，仅装入当前要执行的部分段即可运行，运行过程中若发现访问的页面不在内存，则处理器通知OS按对应的页面置换算法将相应的页面调入到主存，同时OS将暂时不用的页面置换到外存中。 请求分段：在程序开始运行之前，仅装入当前要执行的部分段即可运行，在执行过程中，可使用请求调入中断动态装入要访问但又不在内存的程序段，当内存空间已满，而又需要装入新的段时，根据置换功能适当调出某个段，以便腾出空间而装入新的段。 这里多说一下？很多人容易搞混请求分页与分页存储管理，两者有何不同呢？ 根本区别在于是否将程序的全部地址空间都装入主存； 都需要： 一定容量的内存和外存：在载入程序的时候，只需要将程序的一部分装入内存，而将其他部分留在外存，然后程序就可以执行了； 缺页中断：如果需执行的指令或访问的数据尚未在内存（称为缺页或缺段），则由处理器通知操作系统将相应的页面或段调入到内存，然后继续执行程序； 虚拟内存：逻辑地址到物理地址的变换页面置换算法 最佳置换算法：淘汰之后不会使用或者很久不使用的页面 先进先出置换算法：淘汰最早装入内存的页面 belady异常 最近最久未使用算法：在各页面设置一个装入内存距离当前时间的标记T，淘汰T值最大的页面 最少使用页面置换算法：选择在之前使用最少的页面作为淘汰页 进程管理 为什么要用线程池？ 池化技术 线程池、数据库连接池、Http 连接池 主要是为了减少每次获取资源的消耗，提高对资源的利用率。 线程池提供了一种限制和管理资源（包括执行一个任务）的方式。每个线程池还维护一些基本统计信息，例如已完成任务的数量。 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。","categories":[{"name":"计算机基础","slug":"计算机基础","permalink":"http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"operating System","slug":"operating-System","permalink":"http://example.com/tags/operating-System/"}]},{"title":"Algorithm And Data structure","slug":"Data structure and algorithm","date":"2022-05-16T14:47:49.706Z","updated":"2022-05-16T15:45:50.825Z","comments":true,"path":"2022/05/16/Data structure and algorithm/","link":"","permalink":"http://example.com/2022/05/16/Data%20structure%20and%20algorithm/","excerpt":"","text":"数组链表二分查找 搜索插入位置 注意边界条件 搜索旋转排序数组 II 回溯 子集 II 位运算异或^ 使用异或改变num二进制的第i位 num^(1&lt;&lt;i) 使用异或判断x*y或x&#x2F;y的符号 bool neg &#x3D; ((x^y)&lt;0)且&amp;或|非~ 左&#x2F;右移(&lt;&lt;，&gt;&gt;) 左移n位，乘以2^n 右移n位，除以2^n 高级数据结构线段树 https://leetcode-cn.com/problems/range-sum-query-mutable/solution/","categories":[{"name":"算法","slug":"算法","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"data structure","slug":"data-structure","permalink":"http://example.com/tags/data-structure/"}]},{"title":"C&C++ Notebook","slug":"C&C++ Notebool","date":"2022-05-16T13:18:43.719Z","updated":"2022-05-16T13:20:26.064Z","comments":true,"path":"2022/05/16/C&C++ Notebool/","link":"","permalink":"http://example.com/2022/05/16/C&C++%20Notebool/","excerpt":"","text":"[toc] 语法宏定义的使用 以下使用方法可节约大量代码量 123#define XX()#undef XXlambda表达式 语法： 捕捉列表mutable—&gt;返回值类型{ 函数体 } [捕捉列表]：该列表总是出现在lambda表达式的起始位置，编译器根据[]来判断接下来的代码是否为lambda表达式，捕捉列表能够捕捉当前作用域中的变量，供lambda函数使用。 [val]：表示以传值方式捕捉变量val [&#x3D;]：表示以传值方式捕捉当前作用域中的变量，包括this指针。 [&amp;val]：表示以引用方式传递捕捉变量val。 [&amp;]：表示以引用方式传递捕捉当前作用域中的所有变量，包括this指针。 [this]：表示以传值方式捕捉当前的this指针。 (参数)：参数列表。与普通函数参数列表使用相同。如果不需要传递参数，可以连同”()”一起省略。 mutable：默认情况下，lambda函数总是一个const函数，捕捉的传值参数具有常性，mutable可以取消常性。使用mutable修饰符时，参数列表不能省略，即使参数为空。 —&gt;返回值类型：返回值类型。使用追踪返回类型形式声明函数的返回值类型，没有返回值此部分可省略。返回值类型明确的情况下，也可省略，由编译器推导。 {函数体}：在函数体内除了可以使用参数外，还能使用捕捉的变量。 左值与右值 概念：最简单判断左值、右值的方式是：等号左边的值即左值，等号右边的值即右值 能够取地址的，有名字的就是左值 不能取地址，没有名字的就是右值 右值引用 1T &amp;&amp;a = ReturnRvalue() 除了作为返回值，作为参数也是可以的 右值引用无论作为参数还是返回值，都可以使用临时变量，并且由于其可以窃取临时变量中的内存，导致其效率较高 常量左值引用是万能类型，当参数是常量左值时，我们传入右值也可以；当返回值是右值时，使用常量左值也可以接收。 左值引用无论是作为参数还是返回值，都要求其不能使用临时变量。 当右值引用作为构造函数参数时，这就是所谓的移动构造函数，也就是所谓的移动语义。 移动语义 让函数中的返回的临时对象空间可以不析构，可以重用 移动构造函数 123A(A &amp;&amp; a):b(a.b)&#123; a.b=nullptr;&#125; 与拷贝构造函数不同的是，它接收的是一个右值引用的参数 移动构造函数使用参数a的成员b初始化了本对象的成员b(而不是像构造函数一样需要分配内存，然后再将内容一次拷贝到新分配的内存中)，而a的成员b随后就被置空。 移动构造函数中要避免使用const右值引用 移动语义可以实现高效的swap函数，如下：、 12345678template&lt;class T&gt;void swap(T&amp; a,T&amp; b)&#123; T tmp(move(a)); a = move(b); b = move(tmp); &#125; 上述代码完全避免了资源的释放与申请，从而完成高效置换。 关键字externthread_local 功能：C++11新引入的一种存储类型，会影响变量的存储周期 C++的四种存储周期 automatic static dynamic thread 有且只有thread_local关键字修饰的变量具有线程周期(thread duration) 这些变量(或者说对象）在线程开始的时候被生成，在线程结束的时候被销毁 每一个线程都拥有一个独立的变量实例 可以和static 与 extern关键字联合使用，这将影响变量的链接属性(to adjust linkage)。 哪些变量可以被声明为thread_local？以下3类都是ok的 命名空间下的全局变量 类的static成员变量 本地变量 volatile 功能：每次存储或读取这个变量的时候，都会直接从内存中读取数据，防止编译器可能优化读取和存储，可能暂时使用寄存器中的值，如果这个变量由别的程序更新了的话，将出现不一致的现象易变性 标准库algorithm lower_bound lower_bound的讲解 前提是有序的情况下，lower_bound返回指向第一个值不小于val的位置，也就是返回第一个大于等于val值的位置。（通过二分查找） functional std::function 概述：通用多态函数封装器，存储、复制及调用任何可调用函数、lambda表达式、bind表达式或其他函数对象，还有指向成员函数指针和指向数据成员指针。若 std::function 不含目标，则称它为空。调用空std::function的目标导致抛出std::bad_function_call异常。 模板声明如下： 构造函数支持默认构造（空function，不是一个有效的function）、引用类型、右值引用、拷贝赋值运算符支持的类型也很全。 重载了bool()可以用于判断function是否是有效的 支持交互function 重载了()，支持不定参数 std::bind 概述：bind是一个标准库函数，定义在functional头文件中。可以将bind函数看作一个通用的函数适配器，它接受一个可调用对象，生成新的可调用对象来适应原对象的参数列表。 使用：auto newCallable&#x3D;bind(callable,arg_list); newCallable和callable都是可调用对象，arg_list对应calllable的参数。当我们调用newCallable时，newCallable会调用callable，并传递给它arg_list中的参数 arg_list中的参数可能包含形如_n的占位符，如_1,_2，代表了newCallable中相应位置的参数当我们调用check6(str)的时候，实际会调用check_size(str,6) auto check6&#x3D;bind(check_size,_1,6); 作用： 改变参数个数 bind可以用于改变参数的个数（减少？），如原本check_size要传入两个参数，通过bind我们可以只传入一个参数，另外的参数则在bind时决定。 改变参数位置 bind还可用于改变参数位置，例如，有一个函数bool isGreater(int a,int b)用于判断第一个参数是否大于第二个参数，则auto isSmaller&#x3D;bind(isGreater,_2,_1); 通过改变参数顺序，让bind返回的可调用对象具有相反的含义。isSmaller(1,2)将实际调用isGreater(2,1)，返回true。 设置类成员函数为回调函数 回调函数往往通过函数指针来实现，而类的成员函数，多了一个隐含的参数this，所以直接赋值给函数指针会引起编译报错。通过bind可以解决此问题 memory std::enable_shared_from_this 功能：能让其一个对象（假设其名为t，且已被一个std::shared_ptr对象pt管理）安全地生成其他额外地std::shared_ptr实例（假设名为 pt1, pt2, … ） ，它们与 pt 共享对象 t 的所有权。 用法：若一个类 T 继承 std::enable_shared_from_this ，则会为该类 T 提供成员函数： shared_from_this 。 当 T 类型对象 t 被一个为名为 pt 的 std::shared_ptr 类对象管理时，调用 T::shared_from_this 成员函数，将会返回一个新的 std::shared_ptr 对象，它与 pt 共享 t 的所有权。 为什么使用？ 需要在类对象的内部中获得一个指向当前对象的 shared_ptr 对象。 如果在一个程序中，对象内存的生命周期全部由智能指针来管理。在这种情况下，要在一个类的成员函数中，对外部返回this指针就成了一个很棘手的问题。 使用场景： 当一个类被share_ptr管理，且在类的成员函数里需要把当前类对象作为参数传给其他函数时，这时就需要传递一个指向自身的 share_ptr。 PS： 一般来说，我们不能直接将this指针返回。如果函数将this指针返回到外部某个变量保存，然后这个对象自身已经析构了，但外部变量并不知道，此时如果外部变量再使用这个指针，就会使得程序崩溃。 链接： C++11中enable_shared_from_this的用法解析 std::unique_ptr p 功能：独占资源所有权的指针 简单说，当我们独占资源的所有权的时候，可以使用 std::unique_ptr 对资源进行管理——离开 unique_ptr 对象的作用域时，会自动释放资源。这是很基本的RAII思想。 这个智能指针将拷贝构造函数和赋值运算符重载函数写在了私有下，相当于禁止了拷贝和赋值的操作，也被成为最简单的智能指针。 用法： 成员方法： 为什么使用？ 使用场景 PS 链接 std::shared_ptr p 功能：共享资源所有权的指针 采用的是引用计数原理来实现多个shared_ptr对象之间共享资源： 能够保证共享的资源只会被释放一次 用法 shared_ptr在内部会维护着一份引用计数，用来记录该份资源被几个对象共享。 当一个shared_ptr对象被销毁时（调用析构函数），析构函数内就会将该计数减1。 如果引用计数减为0后，则表示自己是最后一个使用该资源的shared_ptr对象，必须释放资源。 如果引用计数不是0，就说明自己还有其他对象在使用，则不能释放该资源，否则其他对象就成为野指针。 成员方法： 分离关联的原始指针 p.reset():它将引用计数减少1，如果引用计数变为0，则删除指针。 p.use_count(): 返回智能指针对象的引用计数。 实现 因为_ptrcount指向的对象是在堆上，因此所有的线程都能够访问到该资源，多线程在修改_ptrcount时，则会出现线程安全问题，因此需要在修改_prtcount时需要用锁来保证其数据的正确性。 “*”会返回ptr指向的对象，为什么不需要锁对其进行保护？因为ptr返回的对象有可能被读或者被写，这个不是指针内部所考虑的，而是由调用者进行考虑的。 为什么使用？ 使用场景 PS shared_ptr的循环引用 shared_ptr固然好用，但是它也会有问题存在。假设我们要使用定义一个双向链表，如果我们想要让创建出来的链表的节点都定义成shared_ptr智能指针，那么也需要将节点内的_pre和_next都定义成shared_ptr的智能指针。如果定义成普通指针，那么就不能赋值给shared_ptr的智能指针。 链接 智能指针详细解析 std::weak_ptr p 功能：独占资源所有权的指针 weak_ptr对象指向shared_ptr对象时，不会增加shared_ptr中的引用计数 用法： 成员方法： 为什么使用？ 解决循环引用 在定义双向链表或者在二叉树等有多个指针的时候,如果想要将该类型定义成智能指针，那么结构体内的指针需要定义成weak_ptr类型的指针，防止循环引用的出现。· 使用场景 PS 链接 move.h std::foward 功能：用于完美转发的，它会将输入的参数原封不动地传递到下一个函数中 完美转发： 12template&lt;typename T&gt;void IamForwording(T t)&#123; IrunCodeActually(t); &#125; 换一句话说，假定IrunCodeActuall有重载左值有右值版本，那么IamForwording传左值就进入IrunCodeActuall左值版本，传右值就进入IrunCodeActuall右值版本 std::move 功能：将左值强行转换为右值 转化的左值生命周期并没有因这种转换而改变， 用法： 确保使用std::move用于移动语义的变量是一个临时量 1234567891011121314Class Moveable&#123;public: ... Moveable(Moveable &amp;&amp;m) : i(m.i) ,h(move(m.h))&#123; //#1 m.i = nullptr; &#125; int *i;HugeMem h;&#125;Moveable a;Moveable b(a);//错误，a不是临时变量Moveable c(GetTemp()); 移动语义与std::move结合时，要格外注意不要误用，下面是一个错误使用的示例： 1234567int main()&#123; Moveable a; Moveable c(move(a)); cout &lt;&lt; *a.i &lt;&lt; endl; return 0;&#125; a本身是一个左值，但是被move强转为右值，但是a的生命周期又还没有结束，根据上述移动语义的说明，我们可知：a指向i的内存已经被c窃取了，a.i指针指向空，那么一旦输出i的值，那么程序就会出现错误。所以：我们在使用move语义时，一定要确保被强转的左值很快会被析构，否则就会带来隐患。 链接： 右值引用详解 stat.h 相关链接：(29条消息) C语言stat()函数：获取文件状态_道al的博客-CSDN博客_c stat struct stat：文件状态结构体 string.h memchr C 库函数 void *memchr(const void *str, int c, size_t n) 在参数 str 所指向的字符串的前 n 个字节中搜索第一次出现字符 c（一个无符号字符）的位置。 time.h localtime（不可重入函数，非线程安全） &amp; localtime_r（可重入函数，线程安全） 功能：将日历时间转换为断点表示 用法： 123456struct tm tm;time_t time = event-&gt;getTime();localtime_r(&amp;time, &amp;tm);char buf[64];//数将时间格式化为我们想要的格式strftime(buf, sizeof(buf), m_format.c_str(), &amp;tm); 硬件级别 __builtin_expect 将流水线引入cpu，可以提高cpu的效率。更简单的说，让cpu可以预先取出下一条指令，减少cpu等待取指令的耗时，从而可以提供cpu的效率。 如果存在跳转指令，那么预先取出的指令就无用了。cpu在执行当前指令时，从内存中取出了当前指令的下一条指令。执行完当前指令后，cpu发现不是要执行下一条指令,而是执行offset偏移处的指令。cpu只能重新从内存中取出offset偏移处的指令。因此，跳转指令会降低流水线的效率，也就是降低cpu的效率。 在写程序时应该尽量避免跳转语句。那么如何避免跳转语句呢？答案就是使用__builtin_expect。 编译与调试名词解释字面值- 字面值是指在程序中无需变量保存，可直接表示为一个具体的数字或字符串的值。比如在a = b * 2这个语句中，2就是一个字面值，它本身就是一个具体的值 回调函数- 回调函数就是一个被作为参数传递的函数。 RAII思想 资源获取就是初始化 提供了一种资源自动管理的方式 当产生异常、回滚等现象时，RAII可以正确地释放掉资源 在资源的获取到释放之间，我们往往需要使用资源，但常常一些不可预计的异常是在使用过程中产生，就会使资源的释放环节没有得到执行。 RAII的实现原理很简单，利用stack上的临时对象生命期是程序自动管理的这一特点，将我们的资源释放操作封装在一个临时对象中。 它免除了对需要谨慎使用资源时而产生的大量维护代码。在保证资源正确处理的情况下，还使得代码的可读性也提高了不少。 PS词条结构 功能 用法 为什么使用？ 使用场景 PS 链接","categories":[{"name":"语言","slug":"语言","permalink":"http://example.com/categories/%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://example.com/tags/C/"}]},{"title":"C++ 高性能服务器框架","slug":"C++ 高性能服务器框架开发日志","date":"2022-05-16T13:10:12.678Z","updated":"2022-05-16T14:44:05.703Z","comments":true,"path":"2022/05/16/C++ 高性能服务器框架开发日志/","link":"","permalink":"http://example.com/2022/05/16/C++%20%E9%AB%98%E6%80%A7%E8%83%BD%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%A1%86%E6%9E%B6%E5%BC%80%E5%8F%91%E6%97%A5%E5%BF%97/","excerpt":"","text":"[toc] 概述 C++ 高性能服务器框架（High Performance Server&#x2F;HPS） 可作为WEB、数据库、游戏等服务器 框架提供基本的线程、协程库，IO库，TCP、HTTP网络应用接口 日志模块概述 功能：向文件或者控制台输出服务器日志信息 组件 日志等级（LogLevel） unknow、debug、info、warn、error、fatal 6个级别，报告服务器的各种类型的日志信息 日志事件（LogEvent） 成员属性：包含输出每个日志项的所有属性，如下： 日志级别 文件名 文件行号 程序启动依赖的耗时 线程id 协程id 日志时间 线程名称 日志格式器（LogFormatter） 功能：使用log4j格式，对输入的格式字符串进行解析 除使用默认格式外，可自定义日志格式 默认格式 日志格式项 功能：对日志格式%号后的字母进行解析，如%p代表日志级别、%t代表线程id等 日志输出器（LogAppender） 输出到控制台:StdoutLogAppender 输出到文件:FileLogAppender 日志器（Logger） 日志包装器（LogEventWrap） 功能:执行整体日志逻辑 成员属性： 日志事件 日志配置 功能：使用.yml配置文件对日志器属性进行配置管理 结构体： 日志配置 成员属性：日志等级、日志格式、日志输出器、日志名称 日志输出器配置 成员属性：日志等级，日志输出文件、日志格式、日志输出类型 成员方法： Yaml String和日志配置、日志输出器配置之间的相互转换 类图1234567891011121314151617181920212223242526classDiagram LogEvent --&gt; Logger : 关联 Logger --&gt; LogFormatter : 关联 Logger --&gt; LogAppender : 关联 StdoutLogAppender ..|&gt; LogAppender : 实现 FileLogAppender ..|&gt; LogAppender : 实现 FormatterItem *-- LogFormatter : 组合 class Logger&#123; &#125; class LogEvent&#123; &#125; class LogFormatter&#123; &#125; class LogAppender&#123; &#125; class StdoutLogAppender&#123; &#125; class FileLogAppender&#123; &#125; 整体逻辑 日志管理器中有多个日志器，每个日志器可包含多个不同类型或者级别的日志输出器，均使用统一的日志事件 创建日志器,定义日志格式，并创建日志格式器 12345Logger::Logger(const std::string &amp;name): m_name(name), m_level(LogLevel::DEBUG)&#123; //! 创建日志格式器 m_formatter.reset(new LogFormatter(&quot;%d&#123;%Y-%m-%d %H:%M:%S&#125;%T%t%T%N%T%F%T[%p]%T[%c]%T%f:%l%T%m%n&quot;));&#125; 创建日志格式器,解析日志格式，取得日志项数组（存储解析得到的日志项） 12345LogFormatter::LogFormatter(const std::string &amp;pattern): m_pattern(pattern)&#123; //! 对日志格式进行解析 init();&#125; 创建日志事件 对服务器代码运行的文件、所在行、毫秒数、线程ID、协程ID、时间戳、线程名称、日志器、日志等级进行记录 1234LogEvent::LogEvent(std::shared_ptr&lt;Logger&gt; logger, LogLevel::Level level, const char *file, int32_t line, uint32_t elapse, uint32_t thread_id, uint32_t fiber_id, uint64_t time, const std::string &amp;thread_name) : m_file(file), m_line(line), m_elapse(elapse), m_threadId(thread_id), m_fiberId(fiber_id), m_time(time), m_threadName(thread_name), m_logger(logger), m_level(level) &#123; &#125; 创建日志输出器并添加1void Logger::addAppender(LogAppender::ptr appender) 使用日志包装器执行整体逻辑123LogEventWrap::LogEventWrap(LogEvent::ptr e): m_event(e)&#123;&#125; 获取日志事件中的日志器1234LogEventWrap::~LogEventWrap()&#123; m_event-&gt;getLogger()-&gt;log(m_event-&gt;getLevel(), m_event);&#125; 日志器中遍历日志输出器数组12345678910111213141516171819void Logger::log(LogLevel::Level level, LogEvent::ptr event)&#123; if (level &gt;= m_level) &#123; auto self = shared_from_this(); // MutexType::Lock lock(m_mutex); if (!m_appenders.empty()) &#123; for (auto &amp;i : m_appenders) &#123; i-&gt;log(self, level, event); &#125; &#125; else if (m_root) &#123; m_root-&gt;log(level, event); &#125; &#125;&#125; 日志输出器中遍历格式项数组1234567891011121314151617181920212223242526272829303132333435363738//!使用宏定义，定义日志格式容器 static std::map&lt;std::string, std::function&lt;FormatItem::ptr(const std::string &amp;str)&gt;&gt; s_format_items = &#123;#define XX(str, C) \\ &#123; \\#str, [](const std::string &amp;fmt) &#123; return FormatItem::ptr(new C(fmt)); &#125; \\ &#125; XX(m, MessageFormatItem), // m:消息 XX(p, LevelFormatItem), // p:日志级别 XX(r, ElapseFormatItem), // r:累计毫秒数 XX(c, NameFormatItem), // c:日志名称 XX(t, ThreadIdFormatItem), // t:线程id XX(n, NewLineFormatItem), // n:换行 XX(d, DateTimeFormatItem), // d:时间 XX(f, FilenameFormatItem), // f:文件名 XX(l, LineFormatItem), // l:行号 XX(T, TabFormatItem), // T:Tab XX(F, FiberIdFormatItem), // F:协程id XX(N, ThreadNameFormatItem),#undef XX &#125;; //! 格式项 class MessageFormatItem : public LogFormatter::FormatItem &#123; public: MessageFormatItem(const std::string &amp;str = &quot;&quot;) &#123;&#125; void format(std::ostream &amp;os, Logger::ptr logger, LogLevel::Level level, LogEvent::ptr event) override &#123; os &lt;&lt; event-&gt;getContent(); &#125; &#125;; void StdoutLogAppender::log(std::shared_ptr&lt;Logger&gt; logger, LogLevel::Level level, LogEvent::ptr event) &#123; if (level &gt;= m_level) &#123; // MutexType::Lock lock(m_mutex); m_formatter-&gt;format(std::cout, logger, level, event); &#125; &#125; 对每个日志项格式化放入到输出流中12345678std::ostream &amp;LogFormatter::format(std::ostream &amp;ofs, std::shared_ptr&lt;Logger&gt; logger, LogLevel::Level level, LogEvent::ptr event)&#123; for (auto &amp;i : m_items) &#123; i-&gt;format(ofs, logger, level, event); &#125; return ofs;&#125; 最终获取每条日志的字符串输出流1234std::stringstream &amp;LogEventWrap::getSS()&#123; return m_event-&gt;getSS();&#125; 使用流式方式将日志级别level的日志写入到logger123456#define LOG_LEVEL(logger, level) \\ if (logger-&gt;getLevel() &lt;= level) \\ HPS::LogEventWrap(HPS::LogEvent::ptr(new HPS::LogEvent(logger, level, \\ __FILE__, __LINE__, 0, 0, 0, time(0), &quot;&quot;))) \\ .getSS()#define LOG_DEBUG(logger) LOG_LEVEL(logger, HPS::LogLevel::DEBUG) 总结：每个日志器有自己的日志格式（LogFormatter）、日志输出器（LogAppender，输出到控制台StdoutLogAppender&#x2F;文件的日志输出器FileLogAppender），日志格式有输出各种信息的日志项（FormatItem），日志事件定义日志器需要输出的日志项 配置模块概述 配置文件基于Yaml组件 类型转换偏特化模板类（LexicalCast) C++偏特化 Yaml string的不同类型与使用vector，list，map，unordered_map，set，unordered_set等容器存放的数据类型之间的相互转换 配置参数基类（ConfigVarBase） 功能：对配置参数的基本信息进行存储、描述 成员函数 配置参数和字符串的相互转换（虚函数） 返回配置参数数值的类型名称（虚函数） 成员属性： 配置参数名称 配置参数描述 配置参数模板子类，保存对应类型的参数值（ConfigVar） 功能：对配置参数的基本信息、特定类型进行存储、描述 成员属性 配置参数名称 配置参数描述 配置参数类型名称 所有配置项 变更回调数组 成员函数 将参数值转换为Yaml String 从YAML String 转成参数的值 获取当前参数的值、参数值的类型名称 对变更回调数组的增删改查 设置当前参数的值，若参数发生变化，则通知对应的注册回调函数(变更回调函数的作用???已解决:当Yaml 文件参数发生变化时，记录发生变换的过程) 配置参数管理类（Config） 功能： 成员函数： 获取&#x2F;创建对应参数名的配置参数（通过配置参数名以及配置类型，或者只通过配置参数名 ） 使用yaml node初始化配置模块 加载配置文件 返回配置参数基类 遍历配置模块所有配置项整体逻辑 首先通过参数名称、参数值、参数描述创建并添加配置参数子类对象到所有配置项12HPS::ConfigVar&lt;std::vector&lt;int&gt; &gt;::ptr g_int_vec_value_config =HPS::Config::Lookup(&quot;system.int_vec&quot;, std::vector&lt;int&gt;&#123;1,2&#125;, &quot;system int vec&quot;); 从.yml文件中导入根yaml node1YAML::Node root = YAML::LoadFile(&quot;/home/Zone/workspace/HPS/bin/conf/test.yml&quot;); 从yaml root node开始，递归(因为有map类型)提取.yml文件中的所有yaml node存储到list容器中123static void ListAllMember(const std::string &amp;prefix, const YAML::Node &amp;node, std::list&lt;std::pair&lt;std::string, const YAML::Node&gt;&gt; &amp;output) 遍历list容器，将各yaml node转为配置参数12345678910111213141516171819202122232425262728293031void Config::LoadFromYaml(const YAML::Node &amp;root)&#123; std::list&lt;std::pair&lt;std::string, const YAML::Node&gt;&gt; all_nodes; ListAllMember(&quot;&quot;, root, all_nodes); for (auto &amp;i : all_nodes) &#123; std::string key = i.first; if (key.empty()) &#123; continue; &#125; std::transform(key.begin(), key.end(), key.begin(), ::tolower); //! 遍历所有yaml node，通过配置参数名查找到配置参数基类 ConfigVarBase::ptr var = LookupBase(key); if (var) &#123; if (i.second.IsScalar()) &#123; var-&gt;fromString(i.second.Scalar()); &#125; else &#123; std::stringstream ss; ss &lt;&lt; i.second; var-&gt;fromString(ss.str()); &#125; &#125; &#125;&#125; 如何转换？ 4.1 遍历所有yaml node， 4.2 通过配置参数名查找到配置参数基类 1ConfigVarBase::ptr var = LookupBase(key); 4.3 若yaml node为scalar直接转为yaml node；若yaml node非scalar则通过yaml字符串转为yaml node 12345678910if (i.second.IsScalar())&#123; var-&gt;fromString(i.second.Scalar());&#125;else&#123; std::stringstream ss; ss &lt;&lt; i.second; var-&gt;fromString(ss.str());&#125; 4.3 使用类型转换模板类偏特化，将Yaml string转换为存储相应类型的容器，如vector： 12345678910111213141516171819202122232425FromStr()(val);/** * @brief 类型转换模板类片特化(YAML String 转换成 std::vector&lt;T&gt;) /template &lt;class T&gt;class LexicalCast&lt;std::string, std::vector&lt;T&gt;&gt;&#123;public: std::vector&lt;T&gt; operator()(const std::string &amp;v) &#123; YAML::Node node = YAML::Load(v); //! yaml node转换为数组类型 typename std::vector&lt;T&gt; vec; std::stringstream ss; for (size_t i = 0; i &lt; node.size(); ++i) &#123; ss.str(&quot;&quot;); ss &lt;&lt; node[i]; //! 字符串转换为简单类型 vec.push_back(LexicalCast&lt;std::string, T&gt;()(ss.str())); &#125; return vec; &#125;&#125;; 4.4 转换之后，设定模板参数子类对象的值类型为其转换后的容器类型 12345678910111213141516171819202122232425262728293031323334353637383940414243setValue(FromStr()(val));/** * @brief 从YAML String 转成参数的值 * @exception 当转换失败抛出异常 */bool fromString(const std::string &amp;val) override&#123; try &#123; //! 将Yaml格式字符串m_val类型的Yaml Node setValue(FromStr()(val)); &#125; catch (std::exception &amp;e) &#123; LOG_ERROR(LOG_ROOT()) &lt;&lt; &quot;ConfigVar::fromString exception &quot; &lt;&lt; e.what() &lt;&lt; &quot; convert: string to &quot; &lt;&lt; TypeToName&lt;T&gt;() &lt;&lt; &quot; name=&quot; &lt;&lt; m_name &lt;&lt; &quot; - &quot; &lt;&lt; val; &#125; return false;&#125;/** * @brief 设置当前参数的值 * @detail 如果参数的值有发生变化,则通知对应的注册回调函数 */void setValue(const T &amp;v)&#123; &#123; // RWMutexType::ReadLock lock(m_mutex); if (v == m_val) &#123; return; &#125; for (auto &amp;i : m_cbs) &#123; i.second(m_val, v); &#125; &#125; // RWMutexType::WriteLock lock(m_mutex); m_val = v;&#125; 锁模块概述 对semaphore.h信号量，pthread.h锁模块进行封装 组件 信号量（禁止拷贝） 成员属性： 信号量 成员方法： 获取信号量 释放信号量 局部锁模板类 分为局部锁、局部读锁、局部写锁 为了更加同意方便的使用各种类型的锁（类似于多态继承） 成员属性 互斥量 是否上锁 成员方法 加锁 解锁 互斥量（Mutex） 对pthread_mutex进行封装 空锁（NullMutex） 用于调试 读写锁（RWMutex） 对pthread_rwlock进行封装 控制存在读写的临界区 自旋锁（Spinlock） 对pthread_spinlock进行封装 线程不会放弃CPU时间片，而是通过自旋等待锁的释放，就是说，会不停地再次地尝试获取锁，若失败就再次尝试，直到成功为止 优势：在同步代码块内容不复杂，即执行时间很短的情况下，自旋锁用循环去不停地尝试获取锁，让线程始终处于 Runnable 状态，节省了线程状态切换带来的开销 缺点：如果这把锁一直不能被释放，那么这种尝试只是无用的尝试，会白白浪费处理器资源，虽然一开始自旋锁的开销低于线程切换，但是随着时间的增加 适用场景：自旋锁适用于并发度不是特别高的场景，以及临界区比较短小的情况，这样我们可以利用避免线程切换来提高效率。因为如果临界区很大，线程一旦拿到锁，很久才会释放的话，那就不合适用自旋锁，因为自旋会一直占用 CPU 却无法拿到锁，白白消耗资源。 原子锁（CASLock） 对atomic_flag进行封装 将代码和指令对应起来，使得代码执行变为原子操作，相较于互斥锁是操作系统这一层级的，而原子锁是CPU这一层级的 以上锁使用方法，以互斥量为例 1234typedef Mutex MutexType;MutexType mutex;MutexType::Lock lock(mutex)//临界区代码 整体逻辑线程模块概述 对pthread进行封装，向用户提供使用线程的接口 为何不用C++11的线程组件 线程（禁止拷贝） 成员属性： 线程id： 线程结构： 线程执行函数 线程名称 信号量 成员函数 线程的创建与销毁（构造与析构） 返回线程ID、名称 获取当前的线程指针、线程名称，设置当前线程名称（静态） 等待线程执行完成 线程执行函数（静态）整体逻辑 创建线程 1234567891011121314151617Thread::Thread(std::function&lt;void()&gt; cb, const std::string &amp;name) : m_cb(cb), m_name(name)&#123; if (name.empty()) &#123; m_name = &quot;UNKNOW&quot;; &#125; int rt = pthread_create(&amp;m_thread, nullptr, &amp;Thread::run, this); if (rt) &#123; LOG_ERROR(g_logger) &lt;&lt; &quot;pthread_create thread fail, rt=&quot; &lt;&lt; rt &lt;&lt; &quot; name=&quot; &lt;&lt; name; throw std::logic_error(&quot;pthread_create error&quot;); &#125; //! 与下方notify对应 m_semaphore.wait();&#125; 线程执行函数12345678910111213141516void *Thread::run(void *arg)&#123; Thread *thread = (Thread *)arg; t_thread = thread; t_thread_name = thread-&gt;m_name; thread-&gt;m_id = HPS::GetThreadId(); pthread_setname_np(pthread_self(), thread-&gt;m_name.substr(0, 15).c_str()); std::function&lt;void()&gt; cb; cb.swap(thread-&gt;m_cb); //！ 必须在线程所有属性都初始化完毕后才执行线程函数 thread-&gt;m_semaphore.notify(); cb(); return 0;&#125; 回收线程1234567891011121314void Thread::join()&#123; if (m_thread) &#123; int rt = pthread_join(m_thread, nullptr); if (rt) &#123; LOG_ERROR(g_logger) &lt;&lt; &quot;pthread_join thread fail, rt=&quot; &lt;&lt; rt &lt;&lt; &quot; name=&quot; &lt;&lt; m_name; throw std::logic_error(&quot;pthread_join error&quot;); &#125; m_thread = 0; &#125;&#125; 销毁线程1234567Thread::~Thread()&#123; if (m_thread) &#123; pthread_detach(m_thread); &#125;&#125; 协程模块概述- 功能 组件 协程类（Fiber） 成员属性 协程ID、状态 协程上下文 协程运行栈大小、栈指针 协程运行函数 成员函数 协程的创建和销毁 重置协程执行函数,并设置状态 将当前协程切换到运行状态 将当前协程切换到后台 返回协程id、状态 设置、返回当前线程的运行协程（静态） 返回当前所在的协程、总数量（静态） 将当前协程切换到后台,并设置为READY、HOLD状态（静态） 置为READY、HOLD状态后，使用back跳转回线程主协程 在使用call后，将协程置为运行状态，切入到运行协程执行该方法之后的代码 协程执行函数（静态）整体逻辑 创建协程 123456789101112131415161718192021222324252627282930313233343536373839404142434445//！ 有参构造（公共）Fiber::Fiber(std::function&lt;void()&gt; cb, size_t stacksize, bool use_caller) : m_id(++s_fiber_id), m_cb(cb)&#123; //! 分配协程栈指针和大小 ++s_fiber_count; m_stacksize = stacksize ? stacksize : g_fiber_stack_size-&gt;getValue(); m_stack = StackAllocator::Alloc(m_stacksize); //! 初始化协程上下文 if (getcontext(&amp;m_ctx)) &#123; ASSERT2(false, &quot;getcontext&quot;); &#125; //! 将协程栈指针和大小赋值给协程上下文 m_ctx.uc_link = nullptr; m_ctx.uc_stack.ss_sp = m_stack; m_ctx.uc_stack.ss_size = m_stacksize; //! 将协程执行函数分配给协程上下文 if (!use_caller) &#123; makecontext(&amp;m_ctx, &amp;Fiber::MainFunc, 0); &#125; else &#123; makecontext(&amp;m_ctx, &amp;Fiber::CallerMainFunc, 0); &#125; LOG_DEBUG(g_logger) &lt;&lt; &quot;Fiber::Fiber id=&quot; &lt;&lt; m_id;&#125;//! 无参构造创建主协程，所以必须为私有方法，外部不能随意创建Fiber::Fiber()&#123; m_state = EXEC; //! 创建时将本协程设为当前协程 SetThis(this); //! 初始化协程上下文 if (getcontext(&amp;m_ctx)) &#123; ASSERT2(false, &quot;getcontext&quot;); &#125; ++s_fiber_count; LOG_DEBUG(g_logger) &lt;&lt; &quot;Fiber::Fiber main&quot;;&#125; 返回当前协程，若当前线程不存在任何协程，则创建主协程并返回 1234567891011Fiber::ptr Fiber::GetThis() &#123; if (t_fiber) &#123; return t_fiber-&gt;shared_from_this(); &#125; Fiber::ptr main_fiber(new Fiber); ASSERT(t_fiber == main_fiber.get()); t_threadFiber = main_fiber; return t_fiber-&gt;shared_from_this(); &#125; 协程执行 123456789101112131415161718192021222324252627282930313233343536void Fiber::CallerMainFunc()&#123; Fiber::ptr cur = GetThis(); ASSERT(cur); try &#123; cur-&gt;m_cb(); cur-&gt;m_cb = nullptr; //! 运行完毕后，置协程状态为终止 cur-&gt;m_state = TERM; &#125; catch (std::exception &amp;ex) &#123; //! 若捕捉到异常，则置协程状态为异常 cur-&gt;m_state = EXCEPT; LOG_ERROR(g_logger) &lt;&lt; &quot;Fiber Except: &quot; &lt;&lt; ex.what() &lt;&lt; &quot; fiber_id=&quot; &lt;&lt; cur-&gt;getId() &lt;&lt; std::endl &lt;&lt; HPS::BacktraceToString(); &#125; catch (...) &#123; cur-&gt;m_state = EXCEPT; LOG_ERROR(g_logger) &lt;&lt; &quot;Fiber Except&quot; &lt;&lt; &quot; fiber_id=&quot; &lt;&lt; cur-&gt;getId() &lt;&lt; std::endl &lt;&lt; HPS::BacktraceToString(); &#125; auto raw_ptr = cur.get(); //! 首先将cur与原始指针分离 cur.reset(); //! 然后切换回主协程 raw_ptr-&gt;back(); ASSERT2(false, &quot;never reach fiber_id=&quot; + std::to_string(raw_ptr-&gt;getId()));&#125; 主协程切换至运行协程 12345678910void Fiber::call() &#123; SetThis(this); m_state = EXEC; //! 切换协程上下文 if (swapcontext(&amp;t_threadFiber-&gt;m_ctx, &amp;m_ctx)) &#123; ASSERT2(false, &quot;swapcontext&quot;); &#125; &#125; 协程执行 123456789101112131415161718192021222324252627282930313233343536void Fiber::CallerMainFunc()&#123; Fiber::ptr cur = GetThis(); ASSERT(cur); try &#123; cur-&gt;m_cb(); cur-&gt;m_cb = nullptr; //! 运行完毕后，置协程状态为终止 cur-&gt;m_state = TERM; &#125; catch (std::exception &amp;ex) &#123; //! 若捕捉到异常，则置协程状态为异常 cur-&gt;m_state = EXCEPT; LOG_ERROR(g_logger) &lt;&lt; &quot;Fiber Except: &quot; &lt;&lt; ex.what() &lt;&lt; &quot; fiber_id=&quot; &lt;&lt; cur-&gt;getId() &lt;&lt; std::endl &lt;&lt; HPS::BacktraceToString(); &#125; catch (...) &#123; cur-&gt;m_state = EXCEPT; LOG_ERROR(g_logger) &lt;&lt; &quot;Fiber Except&quot; &lt;&lt; &quot; fiber_id=&quot; &lt;&lt; cur-&gt;getId() &lt;&lt; std::endl &lt;&lt; HPS::BacktraceToString(); &#125; auto raw_ptr = cur.get(); //! 首先将cur与原始指针分离 cur.reset(); //! 然后切换回主协程 raw_ptr-&gt;back(); ASSERT2(false, &quot;never reach fiber_id=&quot; + std::to_string(raw_ptr-&gt;getId()));&#125; 协程运行完毕后切换至主协程 123456789void Fiber::back()&#123; SetThis(t_threadFiber.get()); //! 切换协程上下文 if (swapcontext(&amp;m_ctx, &amp;t_threadFiber-&gt;m_ctx)) &#123; ASSERT2(false, &quot;swapcontext&quot;); &#125;&#125; 销毁协程 1234567891011121314151617181920212223242526Fiber::~Fiber()&#123; --s_fiber_count; //! 有协程栈时，说明为其他协程，释放协程栈空间 if (m_stack) &#123; ASSERT(m_state == TERM || m_state == EXCEPT || m_state == INIT); StackAllocator::Dealloc(m_stack, m_stacksize); &#125; //! 没有协程栈时，说明为主协程，将主协程置空 else &#123; //! 主协程没有运行函数且状态一直为执行 ASSERT(!m_cb); ASSERT(m_state == EXEC); Fiber *cur = t_fiber; if (cur == this) &#123; SetThis(nullptr); &#125; &#125; LOG_DEBUG(g_logger) &lt;&lt; &quot;Fiber::~Fiber id=&quot; &lt;&lt; m_id &lt;&lt; &quot; total=&quot; &lt;&lt; s_fiber_count;&#125; 协程调度模块概述 功能：内部有一个线程池,支持协程在线程池里面切换，封装的是N-M的协程调度器，N个线程，M个携程组件 协程调度器类（Scheduler） 成员属性 互斥锁 线程池 待执行的任务队列 调度协程（use_caller为true时有效） 协程调度器名称 以上为公有属性 协程下的线程id数组 线程数量 工作线程数量 空闲线程数量 是否正在停止 是否自动停止 主线程id(use_caller) 成员函数整体逻辑 主线程创建协程调度器12345678910111213141516171819202122232425262728Scheduler::Scheduler(size_t threads, bool use_caller, const std::string &amp;name) : m_name(name) &#123; ASSERT(threads &gt; 0); //! 若需要主线程（创建该调度器的线程，非线程池的线程）执行任务 if (use_caller) &#123; //! 创建线程主协程 HPS::Fiber::GetThis(); --threads; ASSERT(GetThis() == nullptr); t_scheduler = this; //! 回调函数往往通过函数指针来实现，而类的成员函数，多了一个隐含的参数this， //! 所以直接赋值给函数指针会引起编译报错。通过bind可以解决此问题 m_rootFiber.reset(new Fiber(std::bind(&amp;Scheduler::run, this), 0, true)); HPS::Thread::SetName(m_name); //! 线程主协程作为调度协程 t_scheduler_fiber = m_rootFiber.get(); m_rootThread = HPS::GetThreadId(); m_threadIds.push_back(m_rootThread); &#125; else &#123; m_rootThread = -1; &#125; m_threadCount = threads; &#125; 调度器启动12345678910111213141516171819void Scheduler::start()&#123; MutexType::Lock lock(m_mutex); //! 若调度器正处于停止状态，则无法启动 if (!m_stopping) &#123; return; &#125; m_stopping = false; ASSERT(m_threads.empty()); //! 创建线程池 m_threads.resize(m_threadCount); for (size_t i = 0; i &lt; m_threadCount; ++i) &#123; m_threads[i].reset(new Thread(std::bind(&amp;Scheduler::run, this), m_name + &quot;_&quot; + std::to_string(i))); m_threadIds.push_back(m_threads[i]-&gt;getId()); &#125; lock.unlock();&#125; 3)调度器执行 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127void Scheduler::run()&#123; LOG_DEBUG(g_logger) &lt;&lt; m_name &lt;&lt; &quot; run&quot;; // set_hook_enable(true); //! 设置当前线程的调度器（作用???） setThis(); //! 如果当前线程不是主线程 if (HPS::GetThreadId() != m_rootThread) &#123; //! 在该线程上创建调度协程 t_scheduler_fiber = Fiber::GetThis().get(); &#125; //! 运行空闲方法的协程 Fiber::ptr idle_fiber(new Fiber(std::bind(&amp;Scheduler::idle, this))); //! 执行任务的协程 Fiber::ptr cb_fiber; Task task; while (true) &#123; task.reset(); //! 作用（???） bool tickle_me = false; bool is_active = false; &#123; MutexType::Lock lock(m_mutex); auto it = m_tasks.begin(); //! 遍历任务队列， while (it != m_tasks.end()) &#123; //! 若该任务不能被任意线程执行，或者不是能执行该任务相应的线程 if (it-&gt;thread != -1 &amp;&amp; it-&gt;thread != HPS::GetThreadId()) &#123; ++it; tickle_me = true; continue; &#125; ASSERT(it-&gt;fiber || it-&gt;cb); //! 若该任务已经在执行状态 if (it-&gt;fiber &amp;&amp; it-&gt;fiber-&gt;getState() == Fiber::EXEC) &#123; ++it; continue; &#125; //! 线程取得任务 task = *it; m_tasks.erase(it++); ++m_activeThreadCount; is_active = true; break; &#125; tickle_me |= it != m_tasks.end(); &#125; //! ??? if (tickle_me) &#123; tickle(); &#125; //! 取到任务时执行 if (task.fiber &amp;&amp; (task.fiber-&gt;getState() != Fiber::TERM &amp;&amp; task.fiber-&gt;getState() != Fiber::EXCEPT)) &#123; task.fiber-&gt;call(); --m_activeThreadCount; if (task.fiber-&gt;getState() == Fiber::READY) &#123; schedule(task.fiber); &#125; else if (task.fiber-&gt;getState() != Fiber::TERM &amp;&amp; task.fiber-&gt;getState() != Fiber::EXCEPT) &#123; task.fiber-&gt;m_state = Fiber::HOLD; &#125; task.reset(); &#125; else if (task.cb) &#123; if (cb_fiber) &#123; cb_fiber-&gt;reset(task.cb); &#125; else &#123; cb_fiber.reset(new Fiber(task.cb)); &#125; task.reset(); cb_fiber-&gt;call(); --m_activeThreadCount; if (cb_fiber-&gt;getState() == Fiber::READY) &#123; schedule(cb_fiber); cb_fiber.reset(); &#125; else if (cb_fiber-&gt;getState() == Fiber::EXCEPT || cb_fiber-&gt;getState() == Fiber::TERM) &#123; cb_fiber-&gt;reset(nullptr); &#125; else &#123; // if(cb_fiber-&gt;getState() != Fiber::TERM) &#123; cb_fiber-&gt;m_state = Fiber::HOLD; cb_fiber.reset(); &#125; &#125; //! 没有取到时，陷入空闲方法 else &#123; if (is_active) &#123; --m_activeThreadCount; continue; &#125; if (idle_fiber-&gt;getState() == Fiber::TERM) &#123; LOG_INFO(g_logger) &lt;&lt; &quot;idle fiber term&quot;; break; &#125; ++m_idleThreadCount; idle_fiber-&gt;call(); --m_idleThreadCount; if (idle_fiber-&gt;getState() != Fiber::TERM &amp;&amp; idle_fiber-&gt;getState() != Fiber::EXCEPT) &#123; idle_fiber-&gt;m_state = Fiber::HOLD; &#125; &#125; &#125;&#125; 主线程将调度器停止 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081void Scheduler::stop()&#123; m_autoStop = true; //! 当只有主线程执行任务时 if (m_rootFiber &amp;&amp; m_threadCount == 0 &amp;&amp; (m_rootFiber-&gt;getState() == Fiber::TERM || m_rootFiber-&gt;getState() == Fiber::INIT)) &#123; LOG_INFO(g_logger) &lt;&lt; this &lt;&lt; &quot; stopped&quot;; m_stopping = true; if (stopping()) &#123; return; &#125; &#125; // bool exit_on_this_fiber = false; //! 主线程参与执行任务 if (m_rootThread != -1) &#123; ASSERT(GetThis() == this); &#125; //! 主线程不参与执行任务 else &#123; //! 其他线程不得停止调度器 ASSERT(GetThis() != this); &#125; m_stopping = true; for (size_t i = 0; i &lt; m_threadCount; ++i) &#123; tickle(); &#125; if (m_rootFiber) &#123; tickle(); &#125; if (m_rootFiber) &#123; // while(!stopping()) &#123; // if(m_rootFiber-&gt;getState() == Fiber::TERM // || m_rootFiber-&gt;getState() == Fiber::EXCEPT) &#123; // m_rootFiber.reset(new Fiber(std::bind(&amp;Scheduler::run, this), 0, true)); // LOG_INFO(g_logger) &lt;&lt; &quot; root fiber is term, reset&quot;; // t_fiber = m_rootFiber.get(); // &#125; // m_rootFiber-&gt;call(); // &#125; if (!stopping()) &#123; //! 主线程执行任务 m_rootFiber-&gt;call(); &#125; &#125; std::vector&lt;Thread::ptr&gt; thrs; &#123; MutexType::Lock lock(m_mutex); thrs.swap(m_threads); &#125; for (auto &amp;i : thrs) &#123; i-&gt;join(); &#125; &#125;//! 空闲方法，void Scheduler::idle()&#123; LOG_INFO(g_logger) &lt;&lt; &quot;idle&quot;; //! 直到调度器可以终止 while (!stopping()) &#123; //! 空闲协程和调度协程在此切入切出 HPS::Fiber::YieldToHold(); &#125;&#125; 调度器销毁 12345678Scheduler::~Scheduler() &#123; ASSERT(m_stopping); if (GetThis() == this) &#123; t_scheduler = nullptr; &#125; &#125; 测试1234567891011121314151617181920212223static HPS::Logger::ptr g_logger = LOG_ROOT();void test_fiber() &#123; static int s_count = 5; LOG_INFO(g_logger) &lt;&lt; &quot;test in fiber s_count=&quot; &lt;&lt; s_count; sleep(1); if(--s_count &gt;= 0) &#123; HPS::Scheduler::GetThis()-&gt;schedule(&amp;test_fiber, HPS::GetThreadId()); &#125;&#125;int main(int argc, char** argv) &#123; LOG_INFO(g_logger) &lt;&lt; &quot;main&quot;; HPS::Scheduler sc(1, false, &quot;test&quot;); sc.start(); sleep(2); LOG_INFO(g_logger) &lt;&lt; &quot;schedule&quot;; sc.schedule(&amp;test_fiber); sc.stop(); LOG_INFO(g_logger) &lt;&lt; &quot;over&quot;; return 0;&#125; 定时器模块概述 功能： 组件 定时器类（Timer） 成员属性 是否循环定时器 执行周期 精确的执行时间 回调函数 定时器管理器 成员函数 创建、销毁定时器 取消定时器 刷新设置定时器的执行时间 重置定时器时间 定时器管理器类(TimerManager) 成员属性 互斥量 定时器集合 按照执行时间先后排序 是否触发onTimerInsertedAtFront 上次执行时间 成员函数 添加定时器 添加条件定时器 到最近一个定时器执行的时间间隔(毫秒) 获取需要执行的定时器的回调函数列表 是否有定时器 当有新的定时器插入到定时器的首部,执行该函数 将定时器添加到管理器中 检测服务器时间是否被调后了 整体逻辑1)创建定时器 1234567891011Timer::Timer(uint64_t ms, std::function&lt;void()&gt; cb, bool recurring, TimerManager *manager) : m_recurring(recurring), m_ms(ms), m_cb(cb), m_manager(manager) &#123; m_next = HPS::GetCurrentMS() + m_ms; &#125;Timer::Timer(uint64_t next) : m_next(next)&#123;&#125; 创建定时器管理器 1234TimerManager::TimerManager()&#123; m_previouseTime = HPS::GetCurrentMS();&#125; 添加定时器 1234567891011121314151617181920212223Timer::ptr TimerManager::addTimer(uint64_t ms, std::function&lt;void()&gt; cb, bool recurring)&#123; Timer::ptr timer(new Timer(ms, cb, recurring, this)); RWMutexType::WriteLock lock(m_mutex); addTimer(timer, lock); return timer;&#125;void TimerManager::addTimer(Timer::ptr val, RWMutexType::WriteLock &amp;lock)&#123; auto it = m_timers.insert(val).first; //! 若插入的定时器为集合头部（即为最早执行的定时器），且没有触发onTimerInsertedAtFront bool at_front = (it == m_timers.begin()) &amp;&amp; !m_tickled; if (at_front) &#123; m_tickled = true; &#125; lock.unlock(); if (at_front) &#123; onTimerInsertedAtFront(); &#125;&#125; 获取需要执行的定时器的回调函数列表 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253void TimerManager::listExpiredCb(std::vector&lt;std::function&lt;void()&gt;&gt; &amp;cbs)&#123; uint64_t now_ms = HPS::GetCurrentMS(); //! 在定时器集合中提取已经失效的定时器（过期的） std::vector&lt;Timer::ptr&gt; expired; &#123; RWMutexType::ReadLock lock(m_mutex); if (m_timers.empty()) &#123; return; &#125; &#125; RWMutexType::WriteLock lock(m_mutex); if (m_timers.empty()) &#123; return; &#125; //! 检测管理器执行时间是否被调后了,并重置管理器执行时间 bool rollover = detectClockRollover(now_ms); //! 若管理器执行时间没有被调后且最近要执行的定时器时间晚于当前时间 if (!rollover &amp;&amp; ((*m_timers.begin())-&gt;m_next &gt; now_ms)) &#123; //! 不存在过期的定时器 return; &#125; Timer::ptr now_timer(new Timer(now_ms)); //! 若管理器执行时间已调后，则所有定时器全部过期（需要执行） //! 否则将所有精确执行时间为当前时间的定时器放入过期定时器数组 auto it = rollover ? m_timers.end() : m_timers.lower_bound(now_timer); while (it != m_timers.end() &amp;&amp; (*it)-&gt;m_next == now_ms) &#123; ++it; &#125; expired.insert(expired.begin(), m_timers.begin(), it); m_timers.erase(m_timers.begin(), it); cbs.reserve(expired.size()); for (auto &amp;timer : expired) &#123; cbs.push_back(timer-&gt;m_cb); if (timer-&gt;m_recurring) &#123; //! 若为循环定时器，则将定时器重新加入到定时器集合 timer-&gt;m_next = now_ms + timer-&gt;m_ms; m_timers.insert(timer); &#125; else &#123; timer-&gt;m_cb = nullptr; &#125; &#125;&#125; 执行定时器管理器 12345678std::vector&lt;std::function&lt;void()&gt; &gt; cbs;listExpiredCb(cbs);//! 执行定时器管理器if(!cbs.empty()) &#123; //LOG_DEBUG(g_logger) &lt;&lt; &quot;on timer cbs.size=&quot; &lt;&lt; cbs.size(); schedule(cbs.begin(), cbs.end()); cbs.clear();&#125; 6)销毁定时器管理器 123TimerManager::~TimerManager()&#123;&#125; 测试IO协程调度模块概述 功能组件 基于Epoll的IO协程调度器（IOManager） 成员属性 epoll 文件句柄 pipe 文件句柄 当前等待执行的事件数量 IOManager的Mutex socket事件上下文的容器 IO事件 Socket事件管理结构 成员属性 事件上下文：事件执行的调度器、协程、回调函数 读、写事件上下文 事件关联的句柄 当前的事件 事件的Mutex 成员方法 获取事件上下文 重置事件上下文 触发事件 成员函数 创建、销毁IO协程调度 添加、删除、取消、取消所有事件 返回当前的IOManager（静态） 重置socket句柄上下文的容器大小 判断是否可以停止 继承并重写协程调度器的函数：tickle、stopping、idle、onTimerInsertedAtFront 重置socket句柄上下文的容器大小 判断是否可以停止整体逻辑 创建协程调度器 12345678910111213141516171819202122232425IOManager::IOManager(size_t threads, bool use_caller, const std::string &amp;name) : Scheduler(threads, use_caller, name) &#123; //! 创建epoll实例 m_epfd = epoll_create(5000); ASSERT(m_epfd &gt; 0); //! 创建读写管道，作用??? int rt = pipe(m_tickleFds); ASSERT(!rt); //! 向读管道添加epoll读事件，边缘触发 epoll_event event; memset(&amp;event, 0, sizeof(epoll_event)); event.events = EPOLLIN | EPOLLET; event.data.fd = m_tickleFds[0]; //! 设置(F_SETFL)读管道为非阻塞(O_NONBLOCK) rt = fcntl(m_tickleFds[0], F_SETFL, O_NONBLOCK); ASSERT(!rt); //! 向epoll实例添加epoll事件 rt = epoll_ctl(m_epfd, EPOLL_CTL_ADD, m_tickleFds[0], &amp;event); ASSERT(!rt); //! 设定socket事件集合容量 contextResize(32); //! 启动协程调度器 start(); &#125; 线程在空闲方法（协程）中，等待IO事件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149void IOManager::idle()&#123; LOG_DEBUG(g_logger) &lt;&lt; &quot;idle&quot;; const uint64_t MAX_EVNETS = 256; //! 创建epoll事件数组，使用自定义析构函数释放数组空间 epoll_event *events = new epoll_event[MAX_EVNETS](); std::shared_ptr&lt;epoll_event&gt; shared_events(events, [](epoll_event *ptr) &#123; delete[] ptr; &#125;); while (true) &#123; uint64_t next_timeout = 0; //! 调度器可终止时，跳出循环 if (UNLIKELY(stopping(next_timeout))) &#123; LOG_INFO(g_logger) &lt;&lt; &quot;name=&quot; &lt;&lt; getName() &lt;&lt; &quot; idle stopping exit&quot;; break; &#125; int rt = 0; do &#123; //! epoll_wait最长等待时间，超过此事件epoll_wait不再等待，继续执行后继代码 static const int MAX_TIMEOUT = 3000; if (next_timeout != ~0ull) &#123; next_timeout = (int)next_timeout &gt; MAX_TIMEOUT ? MAX_TIMEOUT : next_timeout; &#125; else &#123; next_timeout = MAX_TIMEOUT; &#125; rt = epoll_wait(m_epfd, events, MAX_EVNETS, (int)next_timeout); if (rt &lt; 0 &amp;&amp; errno == EINTR) &#123; &#125; else &#123; break; &#125; &#125; while (true); //! 获取需要执行的定时器的回调函数列表 std::vector&lt;std::function&lt;void()&gt;&gt; cbs; listExpiredCb(cbs); //! 执行定时器管理器 if (!cbs.empty()) &#123; // LOG_DEBUG(g_logger) &lt;&lt; &quot;on timer cbs.size=&quot; &lt;&lt; cbs.size(); schedule(cbs.begin(), cbs.end()); cbs.clear(); &#125; // if(UNLIKELY(rt == MAX_EVNETS)) &#123; // LOG_INFO(g_logger) &lt;&lt; &quot;epoll wait events=&quot; &lt;&lt; rt; // &#125; //! 遍历epoll事件数组 for (int i = 0; i &lt; rt; ++i) &#123; epoll_event &amp;event = events[i]; if (event.data.fd == m_tickleFds[0]) &#123; uint8_t dummy[256]; while (read(m_tickleFds[0], dummy, sizeof(dummy)) &gt; 0) ; continue; &#125; //! 提取epoll事件以及socket文件句柄上的读写事件 FdContext *fd_ctx = (FdContext *)event.data.ptr; FdContext::MutexType::Lock lock(fd_ctx-&gt;mutex); if (event.events &amp; (EPOLLERR | EPOLLHUP)) &#123; event.events |= (EPOLLIN | EPOLLOUT) &amp; fd_ctx-&gt;events; &#125; int real_events = NONE; if (event.events &amp; EPOLLIN) &#123; real_events |= READ; &#125; if (event.events &amp; EPOLLOUT) &#123; real_events |= WRITE; &#125; if ((fd_ctx-&gt;events &amp; real_events) == NONE) &#123; continue; &#125; int left_events = (fd_ctx-&gt;events &amp; ~real_events); int op = left_events ? EPOLL_CTL_MOD : EPOLL_CTL_DEL; event.events = EPOLLET | left_events; int rt2 = epoll_ctl(m_epfd, op, fd_ctx-&gt;fd, &amp;event); if (rt2) &#123; LOG_ERROR(g_logger) &lt;&lt; &quot;epoll_ctl(&quot; &lt;&lt; m_epfd &lt;&lt; &quot;, &quot; &lt;&lt; (EpollCtlOp)op &lt;&lt; &quot;, &quot; &lt;&lt; fd_ctx-&gt;fd &lt;&lt; &quot;, &quot; &lt;&lt; (EPOLL_EVENTS)event.events &lt;&lt; &quot;):&quot; &lt;&lt; rt2 &lt;&lt; &quot; (&quot; &lt;&lt; errno &lt;&lt; &quot;) (&quot; &lt;&lt; strerror(errno) &lt;&lt; &quot;)&quot;; continue; &#125; //! 触发socket句柄上的读写事件 // LOG_INFO(g_logger) &lt;&lt; &quot; fd=&quot; &lt;&lt; fd_ctx-&gt;fd &lt;&lt; &quot; events=&quot; &lt;&lt; fd_ctx-&gt;events // &lt;&lt; &quot; real_events=&quot; &lt;&lt; real_events; if (real_events &amp; READ) &#123; fd_ctx-&gt;triggerEvent(READ); --m_pendingEventCount; &#125; if (real_events &amp; WRITE) &#123; fd_ctx-&gt;triggerEvent(WRITE); --m_pendingEventCount; &#125; &#125; Fiber::ptr cur = Fiber::GetThis(); auto raw_ptr = cur.get(); cur.reset(); raw_ptr-&gt;back(); &#125;&#125;//! 触发事件void IOManager::FdContext::triggerEvent(IOManager::Event event)&#123; // LOG_INFO(g_logger) &lt;&lt; &quot;fd=&quot; &lt;&lt; fd // &lt;&lt; &quot; triggerEvent event=&quot; &lt;&lt; event // &lt;&lt; &quot; events=&quot; &lt;&lt; events; ASSERT(events &amp; event); // if(UNLIKELY(!(event &amp; event))) &#123; // return; // &#125; events = (Event)(events &amp; ~event); EventContext &amp;ctx = getContext(event); if (ctx.cb) &#123; ctx.scheduler-&gt;schedule(&amp;ctx.cb); &#125; else &#123; ctx.scheduler-&gt;schedule(&amp;ctx.fiber); &#125; ctx.scheduler = nullptr; return;&#125; 添加IO任务，建立TCP连接，创建socket事件上下文 123456789101112131415161718//! 添加IO任务iom.schedule(&amp;test_fiber);//! 创建socket事件，建立TCP连接LOG_INFO(g_logger) &lt;&lt; &quot;test_fiber sock=&quot; &lt;&lt; sock;sock = socket(AF_INET, SOCK_STREAM, 0);fcntl(sock, F_SETFL, O_NONBLOCK);sockaddr_in addr;memset(&amp;addr, 0, sizeof(addr));addr.sin_family = AF_INET;addr.sin_port = htons(80);inet_pton(AF_INET, &quot;115.239.210.27&quot;, &amp;addr.sin_addr.s_addr);if (!connect(sock, (const sockaddr *)&amp;addr, sizeof(addr)))&#123;&#125;else if (errno == EINPROGRESS) 向socket上下文添加IO事件上下文 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859int IOManager::addEvent(int fd, Event event, std::function&lt;void()&gt; cb)&#123; //! 扩充socket事件句柄集合容量 FdContext *fd_ctx = nullptr; RWMutexType::ReadLock lock(m_mutex); if ((int)m_fdContexts.size() &gt; fd) &#123; fd_ctx = m_fdContexts[fd]; lock.unlock(); &#125; else &#123; lock.unlock(); RWMutexType::WriteLock lock2(m_mutex); contextResize(fd * 1.5); fd_ctx = m_fdContexts[fd]; &#125; FdContext::MutexType::Lock lock2(fd_ctx-&gt;mutex); if (UNLIKELY(fd_ctx-&gt;events &amp; event)) &#123; LOG_ERROR(g_logger) &lt;&lt; &quot;addEvent assert fd=&quot; &lt;&lt; fd &lt;&lt; &quot; event=&quot; &lt;&lt; (EPOLL_EVENTS)event &lt;&lt; &quot; fd_ctx.event=&quot; &lt;&lt; (EPOLL_EVENTS)fd_ctx-&gt;events; ASSERT(!(fd_ctx-&gt;events &amp; event)); &#125; //! 向epoll实例添加epoll读写事件 int op = fd_ctx-&gt;events ? EPOLL_CTL_MOD : EPOLL_CTL_ADD; epoll_event epevent; epevent.events = EPOLLET | fd_ctx-&gt;events | event; epevent.data.ptr = fd_ctx; int rt = epoll_ctl(m_epfd, op, fd, &amp;epevent); if (rt) &#123; LOG_ERROR(g_logger) &lt;&lt; &quot;epoll_ctl(&quot; &lt;&lt; m_epfd &lt;&lt; &quot;, &quot; &lt;&lt; (EpollCtlOp)op &lt;&lt; &quot;, &quot; &lt;&lt; fd &lt;&lt; &quot;, &quot; &lt;&lt; (EPOLL_EVENTS)epevent.events &lt;&lt; &quot;):&quot; &lt;&lt; rt &lt;&lt; &quot; (&quot; &lt;&lt; errno &lt;&lt; &quot;) (&quot; &lt;&lt; strerror(errno) &lt;&lt; &quot;) fd_ctx-&gt;events=&quot; &lt;&lt; (EPOLL_EVENTS)fd_ctx-&gt;events; return -1; &#125; //! 待处理的读写事件增加 ++m_pendingEventCount; //! 初始化事件上下文 fd_ctx-&gt;events = (Event)(fd_ctx-&gt;events | event); FdContext::EventContext &amp;event_ctx = fd_ctx-&gt;getContext(event); ASSERT(!event_ctx.scheduler &amp;&amp; !event_ctx.fiber &amp;&amp; !event_ctx.cb); event_ctx.scheduler = Scheduler::GetThis(); if (cb) &#123; event_ctx.cb.swap(cb); &#125; else &#123; event_ctx.fiber = Fiber::GetThis(); ASSERT2(event_ctx.fiber-&gt;getState() == Fiber::EXEC, &quot;state=&quot; &lt;&lt; event_ctx.fiber-&gt;getState()); &#125; return 0;&#125; 停止并销毁IO协程调度器123456789101112131415IOManager::~IOManager()&#123; stop(); close(m_epfd); close(m_tickleFds[0]); close(m_tickleFds[1]); for (size_t i = 0; i &lt; m_fdContexts.size(); ++i) &#123; if (m_fdContexts[i]) &#123; delete m_fdContexts[i]; &#125; &#125;&#125; 总结： 首先向调度器添加IO任务（函数或协程），IO任务中会创建socket句柄，向socket事件上下文添加读写事件 调度器中线程池的各线程均陷入空闲协程（idle_fiber），在epoll_wait中等待读写事件的到来 当epoll实例检测到存在读写事件时，线程跳出空闲协程返回调度协程 在调度协程中寻找线程相对应的IO任务，取得后陷入任务协程（task_fiber）执行任务 任务执行完后，回到调度协程循环执行以上流程测试 文件句柄管理模块概述 功能 管理文件句柄类型(是否socket)，是否阻塞，是否关闭，读&#x2F;写超时时间组件 文件句柄上下文类 成员属性 是否初始化 是否socket 是否hook非阻塞 是否用户主动设置非阻塞 是否关闭 文件句柄 读超时时间毫秒 写超时时间毫秒 成员函数 初始化 是否初始化完成 是否socket 是否已关闭 设置用户主动设置非阻塞 获取是否用户主动设置的非阻塞 设置系统非阻塞 获取系统非阻塞 设置超时时间 获取超时时间 文件句柄管理类 成员属性 读写锁 文件句柄集合 成员函数 获取&#x2F;创建文件句柄类FdCtx 删除文件句柄类 整体逻辑测试HOOK模块概述 功能： Hook，就是在一个已有的方法上加入一些钩子，使得在该方法执行前或执行后另在做一些额外的处理 作用 事实上如果一个项目在设计架构时考虑的足够充分，模块抽象的足够合理，设计之初为以后的扩展预留了足够的接口，那么我们完全可以不需要Hook技巧。 但恰恰架构人员在项目设计之初往往没办法想的足够的深远，使得后续在扩展时面临重构的痛苦，这时Hook技巧似乎可以为我们带来一记缓兵之计，通过对旧的架构进行加钩子来满足新的扩展需求。 钩子函数，顾名思义，就是把我们自己实现的hook函数在某一时刻挂接到目标挂载点上。 组件 某某类 成员属性 成员函数整体逻辑 引入外部C库函数，如： 123// readtypedef ssize_t (*read_fun)(int fd, void *buf, size_t count);extern read_fun read_f; 宏定义批量“钩”C库函数 12345678910111213141516171819202122#define HOOK_FUN(XX) \\ XX(sleep) \\ XX(usleep) \\ XX(nanosleep) \\ XX(socket) \\ XX(connect) \\ XX(accept) \\ XX(read) \\ XX(readv) \\ XX(recv) \\ XX(recvfrom) \\ XX(recvmsg) \\ XX(write) \\ XX(writev) \\ XX(send) \\ XX(sendto) \\ XX(sendmsg) \\ XX(close) \\ XX(fcntl) \\ XX(ioctl) \\ XX(getsockopt) \\ XX(setsockopt) 初始化HOOK 1234567891011121314void hook_init() &#123; static bool is_inited = false; if (is_inited) &#123; return; &#125; //! dlsym根据动态链接库操作句柄与符号，返回符号对应的地址， //! 不但可以获取函数地址，可以获取变量地址。 //! 钩住!#define XX(name) name##_f = (name##_fun)dlsym(RTLD_NEXT, #name); HOOK_FUN(XX);#undef XX &#125; HOOK初始化结构体 1234567891011121314struct _HookIniter &#123; _HookIniter() &#123; hook_init(); s_connect_timeout = g_tcp_connect_timeout-&gt;getValue(); g_tcp_connect_timeout-&gt;addListener([](const int &amp;old_value, const int &amp;new_value) &#123; LOG_INFO(g_logger) &lt;&lt; &quot;tcp connect timeout changed from &quot; &lt;&lt; old_value &lt;&lt; &quot; to &quot; &lt;&lt; new_value; s_connect_timeout = new_value; &#125;); &#125; &#125;; 自动创建HOOK初始化结构体默认构造函数 1static _HookIniter s_hook_initer; 自定义IO 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596template &lt;typename OriginFun, typename... Args&gt;static ssize_t do_io(int fd, OriginFun fun, const char *hook_fun_name, uint32_t event, int timeout_so, Args &amp;&amp;...args)&#123; //! 若“不钩”则直接调用内部C库函数 if (!HPS::t_hook_enable) &#123; return fun(fd, std::forward&lt;Args&gt;(args)...); &#125; //! 向句柄管理器添加并获取文件句柄管理对象 HPS::FdCtx::ptr ctx = HPS::FdMgr::GetInstance()-&gt;get(fd); if (!ctx) &#123; return fun(fd, std::forward&lt;Args&gt;(args)...); &#125; //! 文件若关闭 if (ctx-&gt;isClose()) &#123; errno = EBADF; return -1; &#125; //! 不是socket句柄或用户主动设为阻塞 if (!ctx-&gt;isSocket() || ctx-&gt;getUserNonblock()) &#123; return fun(fd, std::forward&lt;Args&gt;(args)...); &#125; //! 获取socket接收或发送的超时时间 uint64_t to = ctx-&gt;getTimeout(timeout_so); std::shared_ptr&lt;timer_info&gt; tinfo(new timer_info);retry: //! foward完美转发参数 ssize_t n = fun(fd, std::forward&lt;Args&gt;(args)...); //! 读写时发生中断 while (n == -1 &amp;&amp; errno == EINTR) &#123; //! 重新调用 n = fun(fd, std::forward&lt;Args&gt;(args)...); &#125; //! 连续做read操作而没有数据可读 if (n == -1 &amp;&amp; errno == EAGAIN) &#123; HPS::IOManager *iom = HPS::IOManager::GetThis(); HPS::Timer::ptr timer; std::weak_ptr&lt;timer_info&gt; winfo(tinfo); if (to != (uint64_t)-1) &#123; timer = iom-&gt;addConditionTimer( to, [winfo, fd, iom, event]() &#123; auto t = winfo.lock(); if(!t || t-&gt;cancelled) &#123; return; &#125; t-&gt;cancelled = ETIMEDOUT; //! 执行并取消事件 iom-&gt;cancelEvent(fd, (HPS::IOManager::Event)(event)); &#125;, winfo); &#125; //! 添加事件的回调为空,默认当前协程为回调 int rt = iom-&gt;addEvent(fd, (HPS::IOManager::Event)(event)); //! 添加事件失败 if (UNLIKELY(rt)) &#123; LOG_ERROR(g_logger) &lt;&lt; hook_fun_name &lt;&lt; &quot; addEvent(&quot; &lt;&lt; fd &lt;&lt; &quot;, &quot; &lt;&lt; event &lt;&lt; &quot;)&quot;; if (timer) &#123; timer-&gt;cancel(); &#125; return -1; &#125; else &#123; //! 添加事件成功 //! 让出协程执行权限??? HPS::Fiber::YieldToHold(); //! 当条件定时器超时时,会唤醒协程,当数据回来时,也会唤醒协程 if (timer) &#123; timer-&gt;cancel(); &#125; //! 通过定时任务唤醒 if (tinfo-&gt;cancelled) &#123; errno = tinfo-&gt;cancelled; return -1; &#125; //! 有IO事件,需要重新读 goto retry; &#125; &#125; return n;&#125; 调用自定义钩子函数 1234ssize_t read(int fd, void *buf, size_t count)&#123; return do_io(fd, read_f, &quot;read&quot;, HPS::IOManager::READ, SO_RCVTIMEO, buf, count);&#125; 网络地址模块功能 通过域名、端口、网卡来获取主机的网络地址等信息类图 123456789101112131415161718192021classDiagram IPAddress --|&gt; Address : 继承 UnixAddress --|&gt; Address : 继承 IPv4Address --|&gt; IPAddress : 继承 IPv6Address --|&gt; IPAddress : 继承 class Address&#123; &#125; class IPAddress&#123; &#125; class UnixAddress&#123; &#125; class IPv4Address&#123; &#125; class IPv6Address&#123; &#125; 组件 网络地址的基类(Address) 成员函数 通过sockaddr指针创建Address 通过host地址返回对应条件的所有Address 通过host地址返回对应条件的任意Address 通过host地址返回对应条件的任意IPAddress 返回本机所有网卡的&lt;网卡名, 地址, 子网掩码位数&gt; 获取指定网卡的地址和子网掩码位数 可读性输出地址 返回协议簇、sockaddr指针,只读、长度、可读性字符串 IP地址的基类(IPAddress)(继承网络地址的基类) 通过域名,IP,服务器名创建IPAddress 获取该地址的广播地址 获取该地址的网段 获取子网掩码地址 返回、设置端口号 IPv4地址(IPv4Addres)(继承IP地址的基类) 使用点分十进制地址创建IPv4Address 通过sockaddr_in构造IPv4Address 通过二进制地址构造IPv4Address IPv6地址(IPv6Address)(继承IP地址的基类) 通过IPv6地址字符串构造IPv6Address 通过sockaddr_in6构造IPv6Address 通过IPv6二进制地址构造IPv6Address UnixSocket地址(UnixAddress)(继承网络地址的基类) 通过路径构造UnixAddress 未知地址(UnknownAddress)(继承网络地址的基类) 整体逻辑 通过域名及端口地址返回对应条件的所有addrinfo结构体12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576bool Address::Lookup(std::vector&lt;Address::ptr&gt; &amp;result, const std::string &amp;host, int family, int type, int protocol) &#123; addrinfo hints, *results, *next; //! 初始化网络地址结构体addrinfo hints.ai_flags = 0; hints.ai_family = family; hints.ai_socktype = type; hints.ai_protocol = protocol; hints.ai_addrlen = 0; //! 主机的规范名称 hints.ai_canonname = NULL; hints.ai_addr = NULL; //! 地址列表中的下一个地址 hints.ai_next = NULL; std::string node; const char *service = NULL; //! 若为 ipv6address serivce如：[1002:003B:456C:678D:890E:0012:234F:56G7]:3306 if (!host.empty() &amp;&amp; host[0] == &#x27;[&#x27;) &#123; const char *endipv6 = (const char *)memchr(host.c_str() + 1, &#x27;]&#x27;, host.size() - 1); if (endipv6) &#123; // TODO check out of range //! service为端口 if (*(endipv6 + 1) == &#x27;:&#x27;) &#123; service = endipv6 + 2; &#125; node = host.substr(1, endipv6 - host.c_str() - 1); &#125; &#125; //! 若为IPv4,不为检查 node serivce if (node.empty()) &#123; service = (const char *)memchr(host.c_str(), &#x27;:&#x27;, host.size()); if (service) &#123; if (!memchr(service + 1, &#x27;:&#x27;, host.c_str() + host.size() - service - 1)) &#123; node = host.substr(0, service - host.c_str()); ++service; &#125; &#125; &#125; //! 没有端口 if (node.empty()) &#123; node = host; &#125; //! 参数分别是：域名，端口，用户设定的 struct addrinfo 结构体，通过result指针参数返回一个指向addrinfo结构体链表的指针 int error = getaddrinfo(node.c_str(), service, &amp;hints, &amp;results); if (error) &#123; LOG_DEBUG(g_logger) &lt;&lt; &quot;Address::Lookup getaddress(&quot; &lt;&lt; host &lt;&lt; &quot;, &quot; &lt;&lt; family &lt;&lt; &quot;, &quot; &lt;&lt; type &lt;&lt; &quot;) err=&quot; &lt;&lt; error &lt;&lt; &quot; errstr=&quot; &lt;&lt; gai_strerror(error); return false; &#125; next = results; //! 递归查询所有有关IP地址结构 while (next) &#123; result.push_back(Create(next-&gt;ai_addr, (socklen_t)next-&gt;ai_addrlen)); // LOG_INFO(g_logger) &lt;&lt; ((sockaddr_in*)next-&gt;ai_addr)-&gt;sin_addr.s_addr; next = next-&gt;ai_next; &#125; //! 释放内存 freeaddrinfo(results); return !result.empty(); &#125; 2.1) 返回本机所有网卡的&lt;网卡名, 地址, 子网掩码位数&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566bool Address::GetInterfaceAddresses(std::multimap&lt;std::string, std::pair&lt;Address::ptr, uint32_t&gt;&gt; &amp;result, int family) &#123; //! 获取本地网络接口的信息。 //! 在路由器上可以用这个接口来获取wan/lan等接口当前的ip地址，广播地址等信息。 struct ifaddrs *next, *results; if (getifaddrs(&amp;results) != 0) &#123; LOG_DEBUG(g_logger) &lt;&lt; &quot;Address::GetInterfaceAddresses getifaddrs &quot; &quot; err=&quot; &lt;&lt; errno &lt;&lt; &quot; errstr=&quot; &lt;&lt; strerror(errno); return false; &#125; try &#123; for (next = results; next; next = next-&gt;ifa_next) &#123; Address::ptr addr; uint32_t prefix_len = ~0u; //! AF_UNSPEC则意味着函数返回的是适用于指定主机名和服务名且适合任何协议族的地址。 if (family != AF_UNSPEC &amp;&amp; family != next-&gt;ifa_addr-&gt;sa_family) &#123; continue; &#125; switch (next-&gt;ifa_addr-&gt;sa_family) &#123; case AF_INET: &#123; //! 创建网络地址基类指针指向不同类型的地址子类 addr = Create(next-&gt;ifa_addr, sizeof(sockaddr_in)); uint32_t netmask = ((sockaddr_in *)next-&gt;ifa_netmask)-&gt;sin_addr.s_addr; //! 返回子网掩码位数 prefix_len = CountBytes(netmask); &#125; break; case AF_INET6: &#123; addr = Create(next-&gt;ifa_addr, sizeof(sockaddr_in6)); in6_addr &amp;netmask = ((sockaddr_in6 *)next-&gt;ifa_netmask)-&gt;sin6_addr; prefix_len = 0; for (int i = 0; i &lt; 16; ++i) &#123; prefix_len += CountBytes(netmask.s6_addr[i]); &#125; &#125; break; default: break; &#125; if (addr) &#123; result.insert(std::make_pair(next-&gt;ifa_name, std::make_pair(addr, prefix_len))); &#125; &#125; &#125; catch (...) &#123; LOG_ERROR(g_logger) &lt;&lt; &quot;Address::GetInterfaceAddresses exception&quot;; freeifaddrs(results); return false; &#125; freeifaddrs(results); return !result.empty(); &#125; 2.2) 通过网卡名称获取指定网卡的地址和子网掩码位数 1234567891011121314151617181920212223242526272829bool Address::GetInterfaceAddresses(std::vector&lt;std::pair&lt;Address::ptr, uint32_t&gt;&gt; &amp;result, const std::string &amp;iface, int family) &#123; if (iface.empty() || iface == &quot;*&quot;) &#123; if (family == AF_INET || family == AF_UNSPEC) &#123; result.push_back(std::make_pair(Address::ptr(new IPv4Address()), 0u)); &#125; if (family == AF_INET6 || family == AF_UNSPEC) &#123; result.push_back(std::make_pair(Address::ptr(new IPv6Address()), 0u)); &#125; return true; &#125; std::multimap&lt;std::string, std::pair&lt;Address::ptr, uint32_t&gt;&gt; results; if (!GetInterfaceAddresses(results, family)) &#123; return false; &#125; auto its = results.equal_range(iface); for (; its.first != its.second; ++its.first) &#123; result.push_back(its.first-&gt;second); &#125; return !result.empty(); &#125; 测试socket封装模块概述 功能 封装socket对用户提供简单易用的API 组件 Socket封装类(Socket) 成员属性(保护) socket句柄 协议簇 类型 TCP类型：SOCK_STREAM UDP类型：SOCK_DGRAM 协议 是否连接 本地地址 远端地址 成员函数(公有) 创建TCP Socket(满足地址类型) 创建UDP Socket(满足地址类型) 创建IPv4&#x2F;IPv6&#x2F;Unix的TCP&#x2F;UDPSocket 构造、析构函数 获取&#x2F;设置 发送&#x2F;接受超时时间(毫秒) 获取&#x2F;设置 sockopt&#x2F;sockopt模板 接收connect链接 绑定(bind)、（重新）连接(connect)、监听(listen)、关闭(socket)、发送(send)、接受(recv) 获取成员属性远端地址、本地地址、协议簇、类型 返回是否连接、是否有效、socket错误、socket句柄 取消读&#x2F;写、accept、所有事件 输出信息到流中 成员函数(保护) 初始化、创建socket 初始化sock SSLSocket封装(继承自Socket)整体逻辑测试 序列化与反序列化模块!!!概述 功能 序列化的原本意图是希望对一个对象作一下“变换”，变成字节序列，这样一来方便持久化存储到磁盘，避免程序运行结束后对象就从内存里消失，另外变换成字节序列也更便于网络运输和传播 网络模块概述 功能 组件HTTP请求&#x2F;响应报文封装 http请求枚举（HTTP_METHOD_MAP(num, name, string)） 编号 名称 http状态枚举（HTTP_STATUS_MAP(code, name, desc)） 状态码 状态名称 描述 通用方法 字符串方法名、字符串指针和HTTP方法枚举的相互转换 获取Map中的key值,并转成对应类型,返回是否成功 HTTP请求结构（HttpRequest） 成员属性（私有） HTTP方法 HTTP版本 是否自动关闭 是否为websocket 请求路径 请求参数 请求fragment 请求消息体 请求头部MAP 请求参数MAP 请求Cookie MAP 成员函数（公有） 构造函数，设置版本、是否keepalive 返回、设置成员属性 设置HTTP请求的方法名、HTTP请求的协议版本、HTTP请求的路径、HTTP请求的查询参数、HTTP请求的Fragment、HTTP请求的消息体 返回、设置是否自动关闭、是否websocket 设置HTTP请求的头部MAP、HTTP请求的参数MAP、HTTP请求的Cookie MAP 获取、设置、删除HTTP请求的头部参数、HTTP请求的请求参数、HTTP请求的Cookie参数 判断HTTP请求的头部参数、请求参数、Cookie参数是否存在 检查并获取HTTP请求的头部参数、获取HTTP请求的头部参数、检查并获取HTTP请求的请求参数、获取HTTP请求的请求参数、检查并获取HTTP请求的Cookie参数、获取HTTP请求的Cookie参数 序列化输出到流中 转成字符串类型 HTTP响应结构体（HttpResponse） 成员属性（私有） 响应状态 版本 是否自动关闭 是否为websocket 响应消息体 响应原因 响应头部MAP 成员函数（公有） 构造函数（版本、是否自动关闭） 返回、设置成员属性、是否自动关闭、是否websocket 获取、设置、删除、检查并获取响应头部参数 序列化输出到流 转成字符串 HTTPSession封装（HttpSession） 成员函数（公有） 构造函数 接收HTTP请求 发送HTTP响应 HTTP请求解析类（HttpRequest） HTTP解析封装 功能 对HTTP的请求行&#x2F;头进行解析 HTTP请求解析类 成员属性（私有） http_parser HttpRequest结构 错误码 成员函数（公有） 构造函数 解析协议 是否解析完成、有错误 返回HttpRequest结构体 设置错误 获取消息体长度 获取http_parser结构体 返回HttpRequest协议解析的缓存大小 返回HttpRequest协议的最大消息体大小 Http响应解析结构体 成员属性（私有） httpclient_parser HttpResponse结构 错误码 成员函数（公有） 构造函数 解析HTTP响应协议 是否解析完成、有错误 返回HttpResponse 设置错误码 获取消息体长度 返回httpclient_parser 返回HTTP响应解析缓存大小 返回HTTP响应最大消息体大小 HTTP解析库http-parser、httpclient_parser http-parser是一个用C编写的HTTP消息解析器，可以解析请求和响应 设计用于高性能HTTP应用程序 它不会进行任何系统调用及内存分配，它不会缓冲数据，它可以被随时中断。根据你的体系结构，每个消息流只需要大约40个字节的数据(在每个连接的web服务器中。它的源码在https://github.com/nodejs/http-parser ，License为MIT，最新发布版为v2.8.1. 不依赖第三方库；处理持续流(keep-alive)；分块解码(decodes chunked encoding)；支持Upgrade；防止缓冲区溢出攻击。 可以从HTTP消息中解析下列信息：报头域及值(Header fields and values)；内容长度(Content-Length)；请求方法；响应状态码；传输编码；HTTP版本；请求URL(网址)；消息体(Message body)。 每个TCP连接使用一个http_parser对象。使用http_parser_init函数初始化结构体并设置回调。 流结构 流类（stream） 成员属性（私有） 成员函数（公有） 读数据 读固定长度的数据 写数据 写固定长度的数据 关闭流 Socket流 Socket流类（SocketStream）继承stream 成员属性（私有） Socket类（Socket::ptr） 是否主控 成员函数（公有） 构造、析构函数 读取数据 写入数据 关闭socket 返回Socket类、是否连接 HTTP Session封装 HTTPSession封装类（HttpSession）继承SocketStream 成员属性（私有） 成员函数（公有） 构造函数 接收HTTP请求 发送HTTP响应 TCP服务器 TCP服务器封装(TcpServer) 成员函数 构造、析构函数 绑定地址 开始接受连接 启动服务 停止服务 返回读取超时时间(毫秒) 返回服务器名称 设置读取超时时间(毫秒) 是否停止 成员属性（私有） 监听Socket数组 新连接的Socket工作的调度器 服务器Socket接收连接的调度器 接收超时时间(毫秒) 服务器名称、类型、服务是否停止 服务器配置 整体逻辑1234567891011121314151617181920212223void run() &#123; //! 返回本机任意网络地址，形成套接字 auto addr = HPS::Address::LookupAny(&quot;0.0.0.0:8033&quot;); //auto addr2 = HPS::UnixAddress::ptr(new HPS::UnixAddress(&quot;/tmp/unix_addr&quot;)); std::vector&lt;HPS::Address::ptr&gt; addrs; addrs.push_back(addr); //addrs.push_back(addr2); //! 创建TCP服务器 HPS::TcpServer::ptr tcp_server(new HPS::TcpServer); std::vector&lt;HPS::Address::ptr&gt; fails; //! 与本机网络地址绑定 while(!tcp_server-&gt;bind(addrs, fails)) &#123; sleep(2); &#125; //! 启动服务器 tcp_server-&gt;start();&#125;int main(int argc, char** argv) &#123; //! 启动IO调度器 HPS::IOManager iom(2); iom.schedule(run); return 0;&#125; Servlet 简介 HttpServlet是Servlet接口的一个实现类，并且它是一个抽象类，servlet.http包中定义了采用HTTP通信协议(一个无状态协议)的HttpServlet类。 响应流程 Web客户向Servlet容器发出Http请求 Servlet容器解析Web客户的Http请求 Servlet容器创建一个HttpRequest对象，在这个对象中封装Http请求信息 Servlet容器创建一个HttpResponse对象 Servlet容器调用HttpServlet的service方法，把HttpRequest和HttpResponse对象作为service方法的参数传给HttpServlet对象 HttpServlet调用HttpRequest的有关方法，获取HTTP请求信息 HttpServlet调用HttpResponse的有关方法，生成响应数据 Servlet容器把HttpServlet的响应结果传给Web客户 其中HttpServlet首先必须读取Http请求的内容，Servlet容器负责创建HttpServlet对象，并把Http请求直接封装到HttpServlet对象中。 创建HttpServlet步骤 继承HttpServlet抽象类 重写HttpServlet的部分方法，如doGet()或doPost()方法 获取HTTP请求信息。通过HttpServletRequest对象来检索HTML表单所提交的数据或URL上的查询字符串 生成HTTP响应结果。通过HttpServletResponse对象生成响应结果 具体实现 HttpServlet的实现由两种方式： 实现方式一：.xml配置实现 配置xml文件(实现方式同Servlet接口的.xml配置相同,此处不再重复) 继承HttpServlet抽象类 重写get()、post()方法 实现方式二：注解实现 重写doGet()和doPost() 设置请求字符集 设置响应对象文本类型 通过请求对象获取用户输入的数据 响应对象通过重定向&#x2F;转发响应用户请求 组件 Servlet封装类（Servlet）（抽象基类） 成员函数 构造&#x2F;析构函数 处理请求 返回Servlet名称 成员属性 名称 函数式Servlet（FunctionServlet） 成员函数 构造函数 成员属性 回调函数 Servlet分发器（ServletDispatch） 成员属性（私有） 读写互斥量 精准匹配servlet MAP 模糊匹配servlet 数组 默认servlet，所有路径都没匹配到时使用 成员函数（公有） 构造函数 添加&#x2F;删除，servlet&#x2F;模糊匹配servlet 返回、设置默认servlet 通过uri获取servlet、模糊匹配servlet 整体逻辑 创建分发器 12345ServletDispatch::ServletDispatch() : Servlet(&quot;ServletDispatch&quot;)&#123; m_default.reset(new NotFoundServlet(&quot;HPS/1.0&quot;));&#125; 处理请求，生成响应（递归） 123456789int32_t ServletDispatch::handle(HPS::http::HttpRequest::ptr request, HPS::http::HttpResponse::ptr response, HPS::http::HttpSession::ptr session)&#123; auto slt = getMatchedServlet(request-&gt;getPath()); if (slt) &#123; slt-&gt;handle(request, response, session); &#125; return 0;&#125; 向服务器添加Servlet 123456789101112131415161718//! 设定0.0.0.0:8020/HPS/xx此路径的消息体sd-&gt;addServlet(&quot;/HPS/xx&quot;, [](HPS::http::HttpRequest::ptr req, HPS::http::HttpResponse::ptr rsp, HPS::http::HttpSession::ptr session) &#123; rsp-&gt;setBody(req-&gt;toString()); return 0; &#125;); void ServletDispatch::addServlet(const std::string &amp;uri, FunctionServlet::callback cb)&#123; RWMutexType::WriteLock lock(m_mutex); //! 这里构造FunctionServlet对象自动触发回调函数cb m_datas[uri] = std::make_shared&lt;HoldServletCreator&gt;( std::make_shared&lt;FunctionServlet&gt;(cb));&#125;FunctionServlet::FunctionServlet(callback cb): Servlet(&quot;FunctionServlet&quot;), m_cb(cb)&#123;&#125; HTTP服务器 HTTP服务器类(HttpServer) 成员函数（公有） 构造函数 获取ServletDispatch 成员函数（保护） 成员属性（私有） 是否支持长连接 Servlet分发器 测试 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556HPS::IOManager::ptr worker;void run()&#123; // g_logger-&gt;setLevel(HPS::LogLevel::INFO); // HPS::http::HttpServer::ptr server(new HPS::http::HttpServer(true, worker.get(), HPS::IOManager::GetThis())); //! 创建HTTP服务器 HPS::http::HttpServer::ptr server(new HPS::http::HttpServer(true)); //! 返回本地主机套接字 HPS::Address::ptr addr = HPS::Address::LookupAnyIPAddress(&quot;0.0.0.0:8020&quot;); //! 绑定 while (!server-&gt;bind(addr)) &#123; sleep(2); &#125; //! 取得服务器Servlet分发器 auto sd = server-&gt;getServletDispatch(); //! 设定0.0.0.0:8020/HPS/xx此路径的消息体 sd-&gt;addServlet(&quot;/HPS/xx&quot;, [](HPS::http::HttpRequest::ptr req, HPS::http::HttpResponse::ptr rsp, HPS::http::HttpSession::ptr session) &#123; rsp-&gt;setBody(req-&gt;toString()); return 0; &#125;); //! 设定0.0.0.0:8020/HPS/此路径的消息体 sd-&gt;addGlobServlet(&quot;/HPS/*&quot;, [](HPS::http::HttpRequest::ptr req, HPS::http::HttpResponse::ptr rsp, HPS::http::HttpSession::ptr session) &#123; rsp-&gt;setBody(&quot;Glob:\\r\\n&quot; + req-&gt;toString()); return 0; &#125;); // sd-&gt;addGlobServlet(&quot;/sylarx/*&quot;, [](HPS::http::HttpRequest::ptr req, HPS::http::HttpResponse::ptr rsp, HPS::http::HttpSession::ptr session) // &#123; // rsp-&gt;setBody(XX(&lt;html&gt; // &lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt; // &lt;body&gt; // &lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt; // &lt;hr&gt;&lt;center&gt;nginx/1.16.0&lt;/center&gt; // &lt;/body&gt; // &lt;/html&gt; // &lt;!-- a padding to disable MSIE and Chrome friendly error page --&gt; // &lt;!-- a padding to disable MSIE and Chrome friendly error page --&gt; // &lt;!-- a padding to disable MSIE and Chrome friendly error page --&gt; // &lt;!-- a padding to disable MSIE and Chrome friendly error page --&gt; // &lt;!-- a padding to disable MSIE and Chrome friendly error page --&gt; // &lt;!-- a padding to disable MSIE and Chrome friendly error page --&gt; // )); // return 0; &#125;); //! 启动HTTP服务器 server-&gt;start();&#125;int main(int argc, char **argv)&#123; HPS::IOManager iom(1); // worker.reset(new HPS::IOManager(3, false, &quot;worker&quot;)); iom.schedule(run); return 0;&#125; ​ PS (!!!)代表可仔细考究， (???)代表不解的问题 词条结构概述 功能组件 某某类 成员属性（私有） 成员函数（公有）整体逻辑测试 报错与调试 在test_scheduler时发生报错 原因：注释掉了Fiber::中的makecontext 需要研究的开发机制协程 为什么需要协程？ 每个线程至少会占用4M的内存空间，10000个线程会消耗39G的内存，而服务器的内存配置只有区区8G 2种选择，一是选择增加服务器，二是选择提高代码效率 操作系统在线程等待IO的时候，会阻塞当前线程，切换到其它线程，当系统线程较少的时候没有什么问题，但是当线程数量非常多的时候，却产生了问题 系统线程会占用非常多的内存空间 过多的线程切换会占用大量的系统时间。 协程刚好可以解决上述2个问题 协程运行在线程之上，当一个协程执行完成后，可以选择主动让出，让另一个协程运行在当前线程之上。 协程并没有增加线程数量，只是在线程的基础之上通过分时复用的方式运行多个协程 协程的切换在用户态完成，切换的代价比线程从用户态到内核态的代价小很多。 我们只需要启动100个线程，每个线程上运行100个协程，这样不仅减少了线程切换开销，而且还能够同时处理10000个读取数据库的任务，很好的解决了上述任务。 协程的注意事项 协程只有在等待IO的过程中才能重复利用线程 假设协程运行在线程之上，并且协程调用了一个阻塞IO操作，这时候会发生什么？ 操作系统并不知道协程的存在，它只知道线程 因此在协程调用阻塞IO操作的时候，操作系统会让线程进入阻塞状态，当前的协程和其它绑定在该线程之上的协程都会陷入阻塞而得不到调度，这往往是不能接受的。 因此在协程中不能调用导致线程阻塞的操作。也就是说，协程只有和异步IO结合起来，才能发挥最大的威力。那么如何处理在协程中调用阻塞IO的操作呢？一般有2种处理方式： epoll 概述： 进程阻塞切换的CPU开销 在高并发的网络IO下，性能的最大绊脚石就是socket在阻塞后导致的进程上下文切换 大约一次进程上下文切换的开销是 3-5 微秒左右。 epoll 作为多路复用技术中的代表，和传统的阻塞网络IO相比，最大的性能提升就是节约掉了大量的进程上下文切换 epoll 内部又涉及出了一套复杂的数据结构，包括一棵红黑树和一个就绪链表 这样应用层和内核态协作的时候就非常的容易了，最少只需要一个进程就可以维护成千上万甚至是百万级别的连接。 这个进程的简单地去就绪队列中查看有没有 Ready，需要被处理的 socket。有就拿走处理。只要活儿足够的多，epoll_wait根本都不会让进程阻塞。用户进程会一直干活，一直干活，直到epoll_wait里实在没活儿可干的时候才主动让出 CPU。大量地减少了进程切换次数 工作原理汇总图 红黑树的作用是仅仅是在管理大量连接的情况下，添加和删除 socket 非常的高效。如果epoll管理的socket固定的话，在数据收发的事件管理过程中其实红黑树是没有起作用的。 内核在socket上收到数据包以后，可以直接找到epitem(epoll item)，并把它插入到就绪队列里，然后等用户进程把事件取走。这个过程中，红黑树的作用并不会得到体现。 原理解析 从网卡接收数据说起 网卡接收数据的过程： 在 1 阶段，网卡收到网线传来的数据。 经过 2 阶段的硬件电路的传输。 最终 3 阶段将数据写入到内存中的某个地址上。 这个过程涉及到DMA传输、IO通路选择等硬件有关的知识，但我们只需知道：网卡会把接收到的数据写入内存。 如何知道接收了数据? 中断 计算机执行程序时，会有优先级的需求。一般而言，由硬件产生的信号需要CPU立马做出回应，不然数据可能就丢失了，所以它的优先级很高。 CPU理应中断掉正在执行的程序，去做出响应;当 CPU 完成对硬件的响应后，再重新执行用户程序。 它和函数调用差不多，只不过函数调用是事先定好位置，而中断的位置由“信号”决定。 当网卡把数据写入到内存后，网卡向CPU发出一个中断信号，操作系统便能得知有新数据到来，再通过网卡中断程序去处理数据。 进程阻塞为什么不占用 CPU 资源? 了解 Epoll 本质的第三步，要从操作系统进程调度的角度来看数据接收。阻塞是进程调度的关键一环，指的是进程在等待某事件(如接收到网络数据)发生之前的等待状态，Recv、Select和Epoll都是阻塞方法。 Recv 是个阻塞方法，当程序运行到Recv时，它会一直等待，直到接收到数据才往下执行。那么阻塞的原理是什么? 工作队列 操作系统为了支持多任务，实现了进程调度的功能，会把进程分为“运行”和“等待”等几种状态 运行状态是进程获得 CPU 使用权，正在执行代码的状态;等待状态是阻塞状态，比如上述程序运行到 Recv 时，程序会从运行状态变为等待状态，接收到数据后又变回运行状态。操作系统会分时执行各个运行状态的进程，由于速度很快，看上去就像是同时执行多个任务。 等待队列 当进程执行到创建 Socket 的语句时，操作系统会创建一个由文件系统管理的 Socket 对象 这个 Socket 对象包含了发送缓冲区、接收缓冲区与等待队列等成员。等待队列是个非常重要的结构，它指向所有需要等待该 Socket 事件的进程。 当程序执行到Recv时，操作系统会将进程A从工作队列移动到该Socket的等待队列中(如下图)。 唤醒进程 当Socket接收到数据后，操作系统将该Socket等待队列上的进程重新放回到工作队列，该进程变成运行状态，继续执行代码。 同时由于 Socket 的接收缓冲区已经有了数据，Recv 可以返回接收到的数据。 内核接收网络数据全过程 计算机收到了对端传送的数据 数据经由网卡传送到内存 然后网卡通过中断信号通知 CPU 有数据到达，CPU 执行中断程序 此处的中断程序主要有两项功能，先将网络数据写入到对应 Socket 的接收缓冲区里面，再唤醒进程 A，重新将进程A放入工作队列中。 以上是内核接收数据全过程，这里我们可能会思考两个问题： 操作系统如何知道网络数据对应于哪个 Socket? 如何同时监视多个Socket的数据? 第一个问题：因为一个Socket对应着一个端口号，而网络数据包中包含了IP和端口的信息，内核可以通过端口号找到对应的 Socket。 第二个问题：是多路复用的重中之重。 同时监视多个Socket的简单方法 服务端需要管理多个客户端连接，而Recv只能监视单个 Socket，这种矛盾下，人们开始寻找监视多个 Socket 的方法。Epoll 的要义就是高效地监视多个 Socket。 假如能够预先传入一个 Socket 列表，如果列表中的 Socket 都没有数据，挂起进程，直到有一个 Socket 收到数据，唤醒进程。这种方法很直接，也是 Select 的设计思想。 select 用法 先准备一个数组 FDS，让 FDS 存放着所有需要监视的 Socket。 然后调用 Select，如果 FDS 中的所有 Socket 都没有数据，Select 会阻塞，直到有一个 Socket 接收到数据，Select 返回，唤醒进程。 用户可以遍历FDS，通过FD_ISSET判断具体哪个Socket收到数据，然后做出处理。 总流程：Select 的实现思路很直接，假如程序同时监视如下图的 Sock1、Sock2 和 Sock3 三个 Socket，那么在调用Select之后，操作系统把进程A分别加入这三个Socket的等待队列中。当任何一个 Socket 收到数据后，中断程序将唤起进程。所谓唤起进程，就是将进程从所有的等待队列中移除，加入到工作队列里。 经由这些步骤，当进程 A 被唤醒后，它知道至少有一个 Socket 接收了数据。程序只需遍历一遍 Socket 列表，就可以得到就绪的 Socket。 select缺点 每次调用 Select都需要将进程加入到所有监视Socket的等待队列，每次唤醒都需要从每个队列中移除。这里涉及了两次遍历，而且每次都要将整个FDS列表传递给内核，有一定的开销。正是因为遍历操作开销大，出于效率的考量，才会规定 Select 的最大监视数量，默认只能监视1024个Socket。 进程被唤醒后，程序并不知道哪些 Socket 收到数据，还需要遍历一次。 Epoll 设计思路 措施一：功能分离 Select 低效的原因之一是将“维护等待队列”和“阻塞进程”两个步骤合二为一。Epoll 拆分了功能 Epoll 将这两个操作分开，先用 epoll_ctl 维护等待队列，再调用 epoll_wait 阻塞进程。显而易见地，效率就能得到提升。 用法： 12345678910111213int s = socket(AF_INET, SOCK_STREAM, 0); bind(s, ...) listen(s, ...) int epfd = epoll_create(...); epoll_ctl(epfd, ...); //将所有需要监听的socket添加到epfd中 while(1)&#123; int n = epoll_wait(...) for(接收到数据的socket)&#123; //处理 &#125; &#125; 就绪列表 Select 低效的另一个原因在于程序不知道哪些Socket收到数据，只能一个个遍历。如果内核维护一个“就绪列表”，引用收到数据的 Socket，就能避免遍历。 就绪列表示意图。如上图所示，计算机共有三个 Socket，收到数据的 Sock2 和 Sock3 被就绪列表 Rdlist 所引用。当进程被唤醒后，只要获取 Rdlist 的内容，就能够知道哪些 Socket 收到数据。 流程 内核创建 eventpoll 对象 eventpoll对象也是文件系统中的一员，和Socket一样，它也会有等待队列。（一个socket有一个等待队列） 创建一个代表该Epoll的eventpoll对象是必须的，因为内核要维护“就绪列表”等数据，“就绪列表”可以作为 eventpoll 的成员。 维护监视列表 创建 Epoll 对象后，可以用 epoll_ctl 添加或删除所要监听的 Socket。以添加 Socket 为例。 如果通过 epoll_ctl 添加 Sock1、Sock2 和 Sock3 的监视，内核会将 eventpoll 添加到这三个 Socket 的等待队列中。 当Socket收到数据后，中断程序会操作 eventpoll 对象，而不是直接操作进程。 接收数据 当 Socket 收到数据后，中断程序会给 eventpoll 的“就绪列表”添加 Socket 引用。 Sock2 和 Sock3 收到数据后，中断程序让 Rdlist 引用这两个 Socket。 eventpoll对象相当于Socket和进程之间的中介，Socket的数据接收并不直接影响进程，而是通过改变eventpoll的就绪列表来改变进程状态。 当程序执行到 epoll_wait 时，如果 Rdlist 已经引用了 Socket，那么 epoll_wait 直接返回，如果 Rdlist 为空，阻塞进程。 阻塞和唤醒进程 假设计算机中正在运行进程A和进程B，在某时刻进程A运行到了epoll_wait语句。 内核会将进程 A 放入 eventpoll 的等待队列中，阻塞进程。 当 Socket 接收到数据，中断程序一方面修改 Rdlist，另一方面唤醒 eventpoll 等待队列中的进程，进程 A 再次进入运行状态，Epoll 唤醒进程 因为Rdlist的存在，进程A可以知道哪些 Socket 发生了变化。 epoll的实现细节 就绪列表的数据结构 就绪列表引用着就绪的 Socket，所以它应能够快速的插入数据。程序可能随时调用 epoll_ctl 添加监视 Socket，也可能随时删除。当删除时，若该Socket已经存放在就绪列表中，它也应该被移除。所以就绪列表应是一种能够快速插入和删除的数据结构。 索引结构 既然 Epoll 将“维护监视队列”和“进程阻塞”分离，也意味着需要有个数据结构来保存监视的 Socket，至少要方便地添加和移除，还要便于搜索，以避免重复添加。 红黑树是一种自平衡二叉查找树**，搜索、插入和删除时间复杂度都是O(log(N))**，效率较好，Epoll 使用了红黑树作为索引结构(对应上图的 RBR)。 因为操作系统要兼顾多种功能，以及有更多需要保存的数据，Rdlist并非直接引用Socket，而是通过Epitem间接引用，红黑树的节点也是Epitem对象。 总结 Epoll在Select和Poll的基础上引入了eventpoll作为中间层，使用了先进的数据结构，是一种高效的多路复用技术。 以表格形式简单对比一下 Select、Poll 与 Epoll 源码分析 假设一个进程保持了10000条连接，那么如何发现哪条连接上有数据可读了、哪条连接可写了？ 采用循环遍历的方式来发现IO事件太低级了 IO多路复用机制更高效 和epoll相关的函数是如下三个： epoll_create：创建一个epoll对象 epoll_ctl：向epoll对象中添加要管理的连接 epoll_wait：等待其管理的连接上的 IO 事件 流程 1）accept创建新socket 我们直接从服务器端的accept讲起。当accept之后，进程会创建一个新的 socket出来，专门用于和对应的客户端通信，然后把它放到当前进程的打开文件列表中。 其中一条连接的socket内核对象更为具体一点的结构图如下。 接收连接时socket内核对象的创建源码。accept的系统调用代码位于源文件net&#x2F;socket.c下 1.1）初始化struct socket对象 调用 sock_alloc 申请一个struct socket对象出来 把 listen 状态的 socket 对象上的协议操作函数集合 ops 赋值给新的 socket。（对于所有的 AF_INET 协议族下的 socket来说，它们的ops方法都是一样的，所以这里可以直接复制过来） 其中 inet_stream_ops 的定义如下 123456789//file: net/ipv4/af_inet.cconst struct proto_ops inet_stream_ops = &#123; ... .accept = inet_accept, .listen = inet_listen, .sendmsg = inet_sendmsg, .recvmsg = inet_recvmsg, ...&#125; 1.2）为新socket对象申请file socket对象中有一个重要的成员，file内核对象指针。在 accept方法里会调用sock_alloc_file来申请内存并初始化。然后将新 file 对象设置到 sock-&gt;file 上。 sock_alloc_file 的实现过程： 123456789struct file *sock_alloc_file(struct socket *sock, int flags, const char *dname)&#123; struct file *file; file = alloc_file(&amp;path, FMODE_READ | FMODE_WRITE, &amp;socket_file_ops); ...... sock-&gt;file = file;&#125; sock_alloc_file又会接着调用到alloc_file。注意在alloc_file 方法中，把 socket_file_ops 函数集合一并赋到了新 file-&gt;f_op 里了。 12345678//file: fs/file_table.cstruct file *alloc_file(struct path *path, fmode_t mode, const struct file_operations *fop)&#123; struct file *file; file-&gt;f_op = fop; ......&#125; 这里看到，在accept里创建的新 socket 里的 file-&gt;f_op-&gt;poll 函数指向的是 sock_poll。接下来我们会调用到它，后面我们再说。 其实 file 对象内部也有一个 socket 指针，指向 socket 对象。 1.3）接收连接 在socket内核对象中除了file对象指针以外，有一个核心成员sock。 这个 struct sock数据结构非常大，是socket的核心内核对象。 发送队列、接收队列、等待队列等核心数据结构都位于此。 sock-&gt;ops-&gt;accept 对应的方法是 inet_accept。它执行的时候会从握手队列里直接获取创建好的 sock。sock 对象的完整创建过程涉及到三次握手 1.4）添加新文件到当前进程的打开文件列表中 2）epoll_create 在用户进程调用 epoll_create 时，内核会创建一个struct event poll的内核对象。并同样把它关联到当前进程的已打开文件列表中。 struct eventpoll 对象，更详细的结构如下 epoll_create 的源代码 1234567891011121314151617181920212223// file：fs/eventpoll.cSYSCALL_DEFINE1(epoll_create1, int, flags)&#123; struct eventpoll *ep = NULL; //创建一个 eventpoll 对象 error = ep_alloc(&amp;ep);&#125;// file：fs/eventpoll.cstruct eventpoll &#123; //sys_epoll_wait用到的等待队列 wait_queue_head_t wq; //接收就绪的描述符都会放到这里 struct list_head rdllist; //每个epoll对象中都有一颗红黑树 struct rb_root rbr; ......&#125; wq： 等待队列链表。软中断数据就绪的时候会通过wq来找到阻塞在epoll对象上的用户进程。 rbr： 一棵红黑树。为了支持对海量连接的高效查找、插入和删除，eventpoll内部使用了一棵红黑树。通过这棵树来管理用户进程下添加进来的所有 socket 连接。 rdllist： 就绪的描述符的链表。当有的连接就绪的时候，内核会把就绪的连接放到 rdllist 链表里。这样应用进程只需要判断链表就能找出就绪进程，而不用去遍历整棵树。 3）epoll_ctl 添加 socket（关键） 在使用 epoll_ctl 注册每一个 socket 的时候，内核会做如下三件事情 分配一个红黑树节点对象epitem， 添加等待事件到socket的等待队列中，其回调函数是ep_poll_callback 将epitem插入到epoll对象的红黑树里 通过 epoll_ctl 添加两个 socket 以后，这些内核数据结构最终在进程中的关系图大致如下： socket 是如何添加到 epoll 对象里的，找到 epoll_ctl 的源码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// file：fs/eventpoll.cSYSCALL_DEFINE4(epoll_ctl, int, epfd, int, op, int, fd, struct epoll_event __user *, event)&#123; struct eventpoll *ep; struct file *file, *tfile; //根据 epfd 找到 eventpoll 内核对象 file = fget(epfd); ep = file-&gt;private_data; //根据 socket 句柄号， 找到其 file 内核对象 tfile = fget(fd); switch (op) &#123; case EPOLL_CTL_ADD: if (!epi) &#123; epds.events |= POLLERR | POLLHUP; error = ep_insert(ep, &amp;epds, tfile, fd); &#125; else error = -EEXIST; clear_tfile_check_list(); break;&#125;//file: fs/eventpoll.cstatic int ep_insert(struct eventpoll *ep, struct epoll_event *event, struct file *tfile, int fd)&#123; //3.1 分配并初始化 epitem //分配一个epi对象 struct epitem *epi; if (!(epi = kmem_cache_alloc(epi_cache, GFP_KERNEL))) return -ENOMEM; //对分配的epi进行初始化 //epi-&gt;ffd中存了句柄号和struct file对象地址 INIT_LIST_HEAD(&amp;epi-&gt;pwqlist); epi-&gt;ep = ep; ep_set_ffd(&amp;epi-&gt;ffd, tfile, fd); //3.2 设置 socket 等待队列 //定义并初始化 ep_pqueue 对象 struct ep_pqueue epq; epq.epi = epi; init_poll_funcptr(&amp;epq.pt, ep_ptable_queue_proc); //调用 ep_ptable_queue_proc 注册回调函数 //实际注入的函数为 ep_poll_callback revents = ep_item_poll(epi, &amp;epq.pt); ...... //3.3 将epi插入到 eventpoll 对象中的红黑树中 ep_rbtree_insert(ep, epi); ......&#125; 3.1） 分配并初始化epitem 对于每一个socket，调用epoll_ctl的时候，都会为之分配一个epitem。该结构的主要数据如下： 123456789101112131415//file: fs/eventpoll.cstruct epitem &#123; //红黑树节点 struct rb_node rbn; //socket文件描述符信息 struct epoll_filefd ffd; //所归属的 eventpoll 对象 struct eventpoll *ep; //等待队列 struct list_head pwqlist;&#125; 对epitem进行了一些初始化，首先在epi-&gt;ep &#x3D; ep这行代码中将其 ep 指针指向 eventpoll 对象。另外用要添加的 socket 的 file、fd 来填充 epitem-&gt;ffd。 其中使用到的 ep_set_ffd 函数如下。 123456static inline void ep_set_ffd(struct epoll_filefd *ffd, struct file *file, int fd)&#123; ffd-&gt;file = file; ffd-&gt;fd = fd;&#125; 3.2）设置 socket 等待队列 在创建epitem并初始化之后，ep_insert中第二件事情就是设置socket对象上的等待任务队列。并把函数 fs&#x2F;eventpoll.c 文件下的 ep_poll_callback 设置为数据就绪时候的回调函数。 在 sock_poll_wait 的第二个参数传参前，先调用了 sk_sleep 函数。在这个函数里它获取了 sock 对象下的等待队列列表头 wait_queue_head_t，待会等待队列项就插入这里。这里稍微注意下，是 socket 的等待队列，不是 epoll 对象的。来看 sk_sleep 源码： 在 ep_ptable_queue_proc 函数中，新建了一个等待队列项，并注册其回调函数为 ep_poll_callback 函数。然后再将这个等待项添加到 socket 的等待队列中。 3.3）分配完 epitem 对象后，紧接着并把它插入到红黑树中。一个插入了一些 socket 描述符的 epoll 里的红黑树的示意图如下： 这里我们再聊聊为啥要用红黑树，很多人说是因为效率高。其实我觉得这个解释不够全面，要说查找效率树哪能比的上 HASHTABLE。我个人认为觉得更为合理的一个解释是为了让 epoll 在查找效率、插入效率、内存开销等等多个方面比较均衡，最后发现最适合这个需求的数据结构是红黑树。 4）epoll_wait 等待接收 epoll_wait 做的事情不复杂，当它被调用时它观察 eventpoll-&gt;rdllist 链表里有没有数据即可。有数据就返回，没有数据就创建一个等待队列项，将其添加到 eventpoll 的等待队列上，然后把自己阻塞掉就完事。 注意：epoll_ctl 添加 socket 时也创建了等待队列项。不同的是这里的等待队列项是挂在 epoll 对象上的，而前者是挂在 socket 对象上的。 12345678910111213141516171819202122232425262728293031//file: fs/eventpoll.cSYSCALL_DEFINE4(epoll_wait, int, epfd, struct epoll_event __user *, events, int, maxevents, int, timeout)&#123; ... error = ep_poll(ep, events, maxevents, timeout);&#125;static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events, int maxevents, long timeout)&#123; wait_queue_t wait; ......fetch_events: //4.1 判断就绪队列上有没有事件就绪 if (!ep_events_available(ep)) &#123; //4.2 定义等待事件并关联当前进程 init_waitqueue_entry(&amp;wait, current); //4.3 把新 waitqueue 添加到 epoll-&gt;wq 链表里 __add_wait_queue_exclusive(&amp;ep-&gt;wq, &amp;wait); for (;;) &#123; ... //4.4 让出CPU 主动进入睡眠状态 if (!schedule_hrtimeout_range(to, slack, HRTIMER_MODE_ABS)) timed_out = 1; ... &#125; 4.1）判断就绪队列上有没有事件就绪 首先调用ep_events_available来判断就绪链表中是否有可处理的事件。 4.2）定义等待事件并关联当前进程 假设确实没有就绪的连接，那接着会进入 init_waitqueue_entry 中定义等待任务，并把 current （当前进程）添加到 waitqueue 上。 是的，当没有IO事件的时候，epoll也是会阻塞掉当前进程。这个是合理的，因为没有事情可做了占着 CPU 也没啥意义。网上的很多文章有个很不好的习惯，讨论阻塞、非阻塞等概念的时候都不说主语。这会导致你看的云里雾里。拿 epoll 来说，epoll 本身是阻塞的，但一般会把 socket 设置成非阻塞。只有说了主语，这些概念才有意义。 epoll_wait 12#include &lt;sys / epoll.h&gt;int epoll_wait（int epfd，struct epoll_event * events， int maxevents，int timeout）; 功能：等待epoll文件描述符上的I &#x2F; O事件。 参数： timeout： timeout参数指定epoll_wait（）将阻止的最小毫秒数。 （此间隔将四舍五入为系统时钟的粒度，并且内核调度延迟意味着阻塞间隔可能会少量溢出。）指定超时值为-1会导致epoll_wait（）无限期阻塞，而指定的超时时间等于零导致epoll_wait（）立即返回，即使没有可用事件。LINUX网络编程Socket getsockopt 12int getsockopt(int socket, int level, int option_name, void *restrict option_value, socklen_t *restrict option_len); 功能：获取一个套接字的选项 参数： socket：文件描述符 level：协议层次 SOL_SOCKET 套接字层次 IPPROTO_IP ip层次 IPPROTO_TCP TCP层次 option_name：选项的名称（套接字层次） SO_BROADCAST 是否允许发送广播信息 SO_REUSEADDR 是否允许重复使用本地地址 SO_SNDBUF 获取发送缓冲区长度 SO_RCVBUF 获取接收缓冲区长度 SO_RCVTIMEO 获取接收超时时间 SO_SNDTIMEO 获取发送超时时间 option_value：获取到的选项的值 option_len：value的长度 文件句柄操作 O_NONBLOCK和O_NDELAY所产生的结果都是使I&#x2F;O变成非阻塞模式(non-blocking)，在读取不到数据或是写入缓冲区已满会马上return，而不会阻塞等待。 在读操作时，如果读不到数据，O_NDELAY会使I&#x2F;O函数马上返回0，但这又衍生出一个问题，因为读取到文件末尾(EOF)时返回的也是0，这样无法区分是哪种情况。因此，O_NONBLOCK就产生出来，它在读取不到数据时会回传-1，并且设置errno为EAGAIN。 信号 EAGIN 这个错误经常出现在当应用程序进行一些非阻塞(non-blocking)操作(对文件或socket)的时候。 例如，以O_NONBLOCK的标志打开文件&#x2F;socket&#x2F;FIFO，如果你连续做read操作而没有数据可读。此时程序不会阻塞起来等待数据准备就绪返回，read函数会返回一个错误EAGAIN，提示你的应用程序现在没有数据可读请稍后再试 又例如，当一个系统调用(比如fork)因为没有足够的资源(比如虚拟内存)而执行失败，返回EAGAIN提示其再调用一次(也许下次就能成功)。 EINTR read（）如果读到数据为0，那么就表示文件结束了，如果在读的过程中遇到了中断那么会返回-1，同时置errno为EINTR。 ETIMEDOUT 连接超时 序列化与反序列化 序列化的原本意图是希望对一个对象作一下“变换”，变成字节序列，这样一来方便持久化存储到磁盘，避免程序运行结束后对象就从内存里消失，另外变换成字节序列也更便于网络运输和传播，所以概念上很好理解： 序列化：把对象转换为字节序列。 反序列化：把字节序列恢复为原先的对象。 对象如何序列化？ 对象的持久化和反持久化需要靠程序员在代码里手动显式地进行序列化和反序列化还原的动作。 序列化 在TCP的连接上，它传输数据的基本形式就是二进制流，也就是一段一段的1和0 在一般编程语言或者网络框架提供的API中，传输数据的基本形式是字节，也就是Byte。一个字节就是8个二进制位，8个Bit。 二进制流和字节流本质上是一样的。对于我们编写的程序来说，它需要通过网络传输的数据是结构化的数据，比如，一条命令、一段文本或者一条消息。对应代码中，这些结构化的数据都可以用一个类或者一个结构体来表示。 序列化的用途除了用于在网络上传输数据以外 将结构化数据保存在文件中（将对象存储于硬盘上），因为文件内保存数据的形式也是二进制序列。 问题：在内存里存放的任何数据，它最基础的存储单元也是二进制比特，也就是说，我们应用程序操作的对象，它在内存中也是使用二进制存储的，既然都是二进制，为什么不能直接把内存中，对象对应的二进制数据直接通过网络发送出去，或者保存在文件中呢？为什么还需要序列化和反序列化呢？ 内存里存的东西，不通用，不同系统，不同语言的组织可能都是不一样的，而且还存在很多引用，指针，并不是直接数据块。内存中的对象数据应该具有语言独特性，例如表达相同业务的User对象(id&#x2F;name&#x2F;age字段),Java和PHP在内存中的数据格式应该不一样的，如果直接用内存中的数据，可能会造成语言不通。只要对序列化的数据格式进行了协商，任何2个语言直接都可以进行序列化传输、接收。 一个数据结构，里面存储的数据是经过非常多其他数据通过非常复杂的算法生成的，因为数据量非常大，因此生成该数据结构所用数据的时间可能要非常久，生成该数据结构后又要用作其他的计算，那么你在调试阶段，每次执行个程序，就光生成数据结构就要花上这么长的时间。假设你确定生成数据结构的算法不会变或不常变，那么就能够通过序列化技术生成数据结构数据存储到磁盘上，下次又一次执行程序时仅仅须要从磁盘上读取该对象数据就可以，所花费时间也就读一个文件的时间。 虽然都是二进制的数据，但是序列化的二进制数据是通过一定的协议将数据字段进行拼接。第一个优势是：不同的语言都可以遵循这种协议进行解析，实现了跨语言。第二个优势是：这种数据可以直接持久化到磁盘，从磁盘读取后也可以通过这个协议解析出来。 定义： 要想使用网络框架的API来传输结构化的数据，必须得先实现结构化的数据与字节流之间的双向转换。这种将结构化数据转换成字节流的过程，称为序列化，反过来转换，就是反序列化。 简单来说，序列化就是将对象实例的状态转换为可保持或传输的格式的过程。与序列化相对的是反序列化，它依据流重构对象。这两个过程结合起来，能够轻松地存储和数据传输。 比如，能够序列化一个对象，然后使用HTTP 通过 Internet 在client和server之间传输该对象。 序列化评价指标 可读性 序列化后的数据最好是易于人类阅读的 实现复杂度 实现的复杂度是否足够低 性能 序列化和反序列化的速度越快越好 信息密度 序列化后的信息密度越大越好，也就是说，同样的一个结构化数据，序列化之后占用的存储空间越小越好","categories":[{"name":"项目","slug":"项目","permalink":"http://example.com/categories/%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://example.com/tags/C/"},{"name":"project","slug":"project","permalink":"http://example.com/tags/project/"}]}],"categories":[{"name":"计算机基础","slug":"计算机基础","permalink":"http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"},{"name":"算法","slug":"算法","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/"},{"name":"语言","slug":"语言","permalink":"http://example.com/categories/%E8%AF%AD%E8%A8%80/"},{"name":"项目","slug":"项目","permalink":"http://example.com/categories/%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"operating System","slug":"operating-System","permalink":"http://example.com/tags/operating-System/"},{"name":"data structure","slug":"data-structure","permalink":"http://example.com/tags/data-structure/"},{"name":"C++","slug":"C","permalink":"http://example.com/tags/C/"},{"name":"project","slug":"project","permalink":"http://example.com/tags/project/"}]}